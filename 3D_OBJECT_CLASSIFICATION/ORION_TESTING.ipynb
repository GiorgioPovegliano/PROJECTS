{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYU0SIRjqfgb"
      },
      "source": [
        "# **Classification of the ModelNet dataset with ORION**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ0S5s-yr0R4"
      },
      "source": [
        "**Installation of the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd8J71-Iqi_t",
        "outputId": "23e451b3-720a-4a5c-fb1e-d467a345bc49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.39.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "#connect to Google drive to access datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#import libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import h5py\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "!pip install torchvision\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "#install wandb in order to obtain graphs\n",
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "# Check for GPU availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8pa0Izfr5mT"
      },
      "source": [
        "\n",
        "#**Data pre-processing**\n",
        "**Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-t8ezbfrA8L"
      },
      "outputs": [],
      "source": [
        "class ImportDatas(Dataset):\n",
        "  def __init__(self, file_txt_path, transform=None):\n",
        "\n",
        "        with open(file_txt_path) as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        self.paths = [r\"/content/drive/MyDrive/ORION_dataset/ModelNet10_24rot/\" + line.strip('\\n')[40:] for line in lines]\n",
        "        self.data, self.label, self.label_pose = self._combine_files()\n",
        "        self.transform = transform\n",
        "\n",
        "  def _combine_files(self):\n",
        "        combined_data = None\n",
        "        combined_label = None\n",
        "        combined_label_pose = None\n",
        "\n",
        "        for path in self.paths:\n",
        "            file = h5py.File(path, 'r')\n",
        "            data = file['data']\n",
        "\n",
        "            label = np.int64(file['label'])\n",
        "            label_pose = np.int64(file['label_pose'])\n",
        "\n",
        "            if combined_data is None:\n",
        "                combined_data = data\n",
        "                combined_label = label\n",
        "                combined_label_pose = label_pose\n",
        "            else:\n",
        "                combined_data = np.concatenate((combined_data, data), axis=0)\n",
        "                combined_label = np.concatenate((combined_label, label), axis=0)\n",
        "                combined_label_pose = np.concatenate((combined_label_pose, label_pose), axis=0)\n",
        "\n",
        "        return combined_data, combined_label, combined_label_pose\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        data = self.data[idx]\n",
        "        label = self.label[idx]\n",
        "        label_pose = self.label_pose[idx]\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "        return data, label, label_pose\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xhw8vJOTrdV_"
      },
      "outputs": [],
      "source": [
        "class CropTransform(torch.nn.Module):\n",
        "  def forward(self, datas):\n",
        "    #cropping datas from [36x36x36] to [32x32x32]\n",
        "    cropped_datas = datas[: ,2:34, 2:34, 2:34]\n",
        "    return cropped_datas\n",
        "\n",
        "class ToTensor(torch.nn.Module):\n",
        "  def forward(self, datas):\n",
        "    tensor = torch.from_numpy(datas.astype(np.float32)) #transform datas into tensor\n",
        "    return tensor\n",
        "\n",
        "transforms = Compose([\n",
        "    CropTransform(),\n",
        "    ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wot6FNfsEeJ"
      },
      "source": [
        "**Datas uploading from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaH-mnukrvD2"
      },
      "outputs": [],
      "source": [
        "#define the type of set poseplan\n",
        "num_classes=10\n",
        "num_poses=210\n",
        "\n",
        "\n",
        "# initialize paths\n",
        "train_dataset = ImportDatas(r'/content/drive/MyDrive/ORION_dataset/ModelNet10_24rot/poseplan_MN10_24/hdf5/train_allrot/train.hdf5.txt', transform=transforms)\n",
        "validation_dataset = ImportDatas(r'/content/drive/MyDrive/ORION_dataset/ModelNet10_24rot/poseplan_MN10_24/hdf5/validation_allrot/train.hdf5.txt', transform=transforms)\n",
        "test_dataset = ImportDatas(r'/content/drive/MyDrive/ORION_dataset/ModelNet10_24rot/poseplan_MN10_24/hdf5/test_allrot/train.hdf5.txt', transform=transforms)\n",
        "\n",
        "# load datas\n",
        "#change batch size for validation and test. 12 if dealing with 12 rotations, 24 for 24 rotations\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8, drop_last=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=24, shuffle=False, num_workers=8, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=24, shuffle=False, num_workers=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmuqMUL0sxQ_"
      },
      "source": [
        "#**Print one object with label**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_zoSojLuENq"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "iterator = tqdm(validation_dataloader, disable=True)\n",
        "torch.set_printoptions(threshold=327680)\n",
        "for datas, label_class, label_pose in iterator:\n",
        "        datas = datas.squeeze().to(device)\n",
        "        label_class = label_class.squeeze().to(device)\n",
        "        label_pose = label_pose.squeeze().to(device)\n",
        "        print(\"Batch Data Shape:\", datas)\n",
        "        #print(\"Batch Class Labels Shape:\", label_class.shape)\n",
        "        #print(\"Batch Pose Labels Shape:\", label_pose.shape)\n",
        "\n",
        "        print(label_class)\n",
        "        print(label_pose)\n",
        "\n",
        "# Add this after initializing Data Logging\n",
        "#print(\"Length of Train Dataset:\", len(train_dataset))\n",
        "print(\"Length of Validation Dataset:\", len(validation_dataset))\n",
        "#print(\"Length of Test Dataset:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q1hcPs_tCOt"
      },
      "source": [
        "# **ORION nets**\n",
        "**Definition of the ORION net with the same architecture provided into the paper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWt8qhNntSYm"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module, Sequential, Conv3d, BatchNorm3d, ReLU, Dropout3d, MaxPool3d, Linear, LeakyReLU, Dropout, ELU\n",
        "\n",
        "# Define a simple CNN architecture\n",
        "class ORION(Module):\n",
        "    def __init__(self, num_classes, num_poses):\n",
        "        super().__init__()\n",
        "        self.model = Sequential(\n",
        "          # Definition of Conv1\n",
        "          Conv3d(in_channels=1, out_channels=32, kernel_size=5, stride=2),\n",
        "          BatchNorm3d(num_features=32),\n",
        "          ReLU(),\n",
        "          Dropout3d(p=0.2),\n",
        "\n",
        "          # Definition of Conv2\n",
        "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=64),\n",
        "          ReLU(),\n",
        "          MaxPool3d(kernel_size=2, stride=2),\n",
        "          Dropout3d(p=0.3),\n",
        "          )\n",
        "\n",
        "        self.fc1 = Sequential(\n",
        "            Linear(64*6*6*6, out_features=128),\n",
        "            ReLU(),\n",
        "            Dropout(p=0.4)\n",
        "        )\n",
        "        self.class_layer = Linear(128, num_classes)\n",
        "        self.pose_layer = Linear(128, num_poses)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x).reshape((x.shape[0], -1))\n",
        "        x = self.fc1(x)\n",
        "        class_output = self.class_layer(x)\n",
        "        pose_output = self.pose_layer(x)\n",
        "        return class_output, pose_output\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, std=0.01)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv3d):\n",
        "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
        "        torch.nn.init.normal_(self.pose_layer.weight, std=0.01)\n",
        "\n",
        "        if self.class_layer.bias is not None:\n",
        "            self.class_layer.bias.data.zero_()\n",
        "\n",
        "        if self.pose_layer.bias is not None:\n",
        "            self.pose_layer.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwmh9bUQtmQq"
      },
      "source": [
        "**Modified versions of the ORION net**\n",
        "\n",
        "**Extended ORION (from Paper)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG9hazq5tqqJ"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN architecture\n",
        "class EXTENDED_ORION(Module):\n",
        "    def __init__(self, num_classes, num_poses):\n",
        "        super().__init__()\n",
        "        self.model = Sequential(\n",
        "          # Definition of Conv1\n",
        "          Conv3d(in_channels=1, out_channels=32, kernel_size=3, stride=2),\n",
        "          BatchNorm3d(num_features=32),\n",
        "          ReLU(),\n",
        "          Dropout3d(p=0.2),\n",
        "\n",
        "          # Definition of Conv2\n",
        "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=64),\n",
        "          ReLU(),\n",
        "          Dropout3d(p=0.3),\n",
        "\n",
        "          # Definition of Conv3\n",
        "          Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=128),\n",
        "          ReLU(),\n",
        "          Dropout3d(p=0.4),\n",
        "\n",
        "          # Definition of Conv4\n",
        "          Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=256),\n",
        "          ReLU(),\n",
        "          MaxPool3d(kernel_size=2, stride=2),\n",
        "          Dropout3d(p=0.6),\n",
        "          )\n",
        "\n",
        "        self.fc1 = Sequential(\n",
        "            Linear(256*4*4*4, out_features=128),\n",
        "            ReLU(),\n",
        "            Dropout(p=0.4)\n",
        "        )\n",
        "        self.class_layer = Linear(128, num_classes)\n",
        "        self.pose_layer = Linear(128, num_poses)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x).reshape((x.shape[0], -1))\n",
        "        x = self.fc1(x)\n",
        "        class_output = self.class_layer(x)\n",
        "        pose_output = self.pose_layer(x)\n",
        "        return class_output, pose_output\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, std=0.01)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv3d):\n",
        "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
        "        torch.nn.init.normal_(self.pose_layer.weight, std=0.01)\n",
        "\n",
        "        if self.class_layer.bias is not None:\n",
        "            self.class_layer.bias.data.zero_()\n",
        "\n",
        "        if self.pose_layer.bias is not None:\n",
        "            self.pose_layer.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnFHls8TJnRE"
      },
      "source": [
        "**Using Leaky Relu as activation function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLbQzCVRIbK8"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN architecture\n",
        "class ORION_LRELU(Module):\n",
        "    def __init__(self, num_classes, num_poses):\n",
        "        super().__init__()\n",
        "        self.model = Sequential(\n",
        "          # Definition of Conv1\n",
        "          Conv3d(in_channels=1, out_channels=32, kernel_size=5, stride=2),\n",
        "          BatchNorm3d(num_features=32),\n",
        "          LeakyReLU(negative_slope=0.1),\n",
        "          Dropout3d(p=0.2),\n",
        "\n",
        "          # Definition of Conv2\n",
        "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=64),\n",
        "          LeakyReLU(negative_slope=0.1),\n",
        "          MaxPool3d(kernel_size=2, stride=2),\n",
        "          Dropout3d(p=0.3),\n",
        "          )\n",
        "\n",
        "        self.fc1 = Sequential(\n",
        "            Linear(64*6*6*6, out_features=128),\n",
        "            ReLU(),\n",
        "            Dropout(p=0.4)\n",
        "        )\n",
        "        self.class_layer = Linear(128, num_classes)\n",
        "        self.pose_layer = Linear(128, num_poses)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x).reshape((x.shape[0], -1))\n",
        "        x = self.fc1(x)\n",
        "        class_output = self.class_layer(x)\n",
        "        pose_output = self.pose_layer(x)\n",
        "        return class_output, pose_output\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, std=0.01)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv3d):\n",
        "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
        "        torch.nn.init.normal_(self.pose_layer.weight, std=0.01)\n",
        "\n",
        "        if self.class_layer.bias is not None:\n",
        "            self.class_layer.bias.data.zero_()\n",
        "\n",
        "        if self.pose_layer.bias is not None:\n",
        "            self.pose_layer.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_ByOmzbI3Yf"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN architecture\n",
        "class EXTENDED_ORION_LRELU(Module):\n",
        "    def __init__(self, num_classes, num_poses):\n",
        "        super().__init__()\n",
        "        self.model = Sequential(\n",
        "          # Definition of Conv1\n",
        "          Conv3d(in_channels=1, out_channels=32, kernel_size=3, stride=2),\n",
        "          BatchNorm3d(num_features=32),\n",
        "          LeakyReLU(negative_slope=0.1),\n",
        "          Dropout3d(p=0.2),\n",
        "\n",
        "          # Definition of Conv2\n",
        "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=64),\n",
        "          LeakyReLU(negative_slope=0.1),\n",
        "          Dropout3d(p=0.3),\n",
        "\n",
        "          # Definition of Conv3\n",
        "          Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=128),\n",
        "          LeakyReLU(negative_slope=0.1),\n",
        "          Dropout3d(p=0.4),\n",
        "\n",
        "          # Definition of Conv4\n",
        "          Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=256),\n",
        "          LeakyReLU(negative_slope=0.1),\n",
        "          MaxPool3d(kernel_size=2, stride=2),\n",
        "          Dropout3d(p=0.6),\n",
        "          )\n",
        "\n",
        "        self.fc1 = Sequential(\n",
        "            Linear(256*4*4*4, out_features=128),\n",
        "            ReLU(),\n",
        "            Dropout(p=0.4)\n",
        "        )\n",
        "        self.class_layer = Linear(128, num_classes)\n",
        "        self.pose_layer = Linear(128, num_poses)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x).reshape((x.shape[0], -1))\n",
        "        x = self.fc1(x)\n",
        "        class_output = self.class_layer(x)\n",
        "        pose_output = self.pose_layer(x)\n",
        "        return class_output, pose_output\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, std=0.01)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv3d):\n",
        "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
        "        torch.nn.init.normal_(self.pose_layer.weight, std=0.01)\n",
        "\n",
        "        if self.class_layer.bias is not None:\n",
        "            self.class_layer.bias.data.zero_()\n",
        "\n",
        "        if self.pose_layer.bias is not None:\n",
        "            self.pose_layer.bias.data.zero_()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJGJsPyOJv39"
      },
      "source": [
        "**Using Exponential- linear unit as activation function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyO6Wu4eJStM"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN architecture\n",
        "class ORION_ELU(Module):\n",
        "    def __init__(self, num_classes, num_poses):\n",
        "        super().__init__()\n",
        "        self.model = Sequential(\n",
        "          # Definition of Conv1\n",
        "          Conv3d(in_channels=1, out_channels=32, kernel_size=5, stride=2),\n",
        "          BatchNorm3d(num_features=32),\n",
        "          ELU(),\n",
        "          Dropout3d(p=0.2),\n",
        "\n",
        "          # Definition of Conv2\n",
        "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=64),\n",
        "          ELU(),\n",
        "          MaxPool3d(kernel_size=2, stride=2),\n",
        "          Dropout3d(p=0.3),\n",
        "          )\n",
        "\n",
        "        self.fc1 = Sequential(\n",
        "            Linear(64*6*6*6, out_features=128),\n",
        "            ReLU(),\n",
        "            Dropout(p=0.4)\n",
        "        )\n",
        "        self.class_layer = Linear(128, num_classes)\n",
        "        self.pose_layer = Linear(128, num_poses)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x).reshape((x.shape[0], -1))\n",
        "        x = self.fc1(x)\n",
        "        class_output = self.class_layer(x)\n",
        "        pose_output = self.pose_layer(x)\n",
        "        return class_output, pose_output\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, std=0.01)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv3d):\n",
        "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
        "        torch.nn.init.normal_(self.pose_layer.weight, std=0.01)\n",
        "\n",
        "        if self.class_layer.bias is not None:\n",
        "            self.class_layer.bias.data.zero_()\n",
        "\n",
        "        if self.pose_layer.bias is not None:\n",
        "            self.pose_layer.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9dPMjFyJcB_"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN architecture\n",
        "class EXTENDED_ORION_ELU(Module):\n",
        "    def __init__(self, num_classes, num_poses):\n",
        "        super().__init__()\n",
        "        self.model = Sequential(\n",
        "          # Definition of Conv1\n",
        "          Conv3d(in_channels=1, out_channels=32, kernel_size=3, stride=2),\n",
        "          BatchNorm3d(num_features=32),\n",
        "          ELU(),\n",
        "          Dropout3d(p=0.2),\n",
        "\n",
        "          # Definition of Conv2\n",
        "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=64),\n",
        "          ELU(),\n",
        "          Dropout3d(p=0.3),\n",
        "\n",
        "          # Definition of Conv3\n",
        "          Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=128),\n",
        "          ELU(),\n",
        "          Dropout3d(p=0.4),\n",
        "\n",
        "          # Definition of Conv4\n",
        "          Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=256),\n",
        "          ELU(),\n",
        "          MaxPool3d(kernel_size=2, stride=2),\n",
        "          Dropout3d(p=0.6),\n",
        "          )\n",
        "\n",
        "        self.fc1 = Sequential(\n",
        "            Linear(256*4*4*4, out_features=128),\n",
        "            ReLU(),\n",
        "            Dropout(p=0.4)\n",
        "        )\n",
        "        self.class_layer = Linear(128, num_classes)\n",
        "        self.pose_layer = Linear(128, num_poses)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x).reshape((x.shape[0], -1))\n",
        "        x = self.fc1(x)\n",
        "        class_output = self.class_layer(x)\n",
        "        pose_output = self.pose_layer(x)\n",
        "        return class_output, pose_output\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, std=0.01)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv3d):\n",
        "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
        "        torch.nn.init.normal_(self.pose_layer.weight, std=0.01)\n",
        "\n",
        "        if self.class_layer.bias is not None:\n",
        "            self.class_layer.bias.data.zero_()\n",
        "\n",
        "        if self.pose_layer.bias is not None:\n",
        "            self.pose_layer.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwOOH6QHjHDa"
      },
      "source": [
        "**Only classes evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C0DnZ2ZjNOW"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN architecture\n",
        "class ORION_OC(Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = Sequential(\n",
        "          # Definition of Conv1\n",
        "          Conv3d(in_channels=1, out_channels=32, kernel_size=5, stride=2),\n",
        "          BatchNorm3d(num_features=32),\n",
        "          ReLU(),\n",
        "          Dropout3d(p=0.2),\n",
        "\n",
        "          # Definition of Conv2\n",
        "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=64),\n",
        "          ReLU(),\n",
        "          MaxPool3d(kernel_size=2, stride=2),\n",
        "          Dropout3d(p=0.3),\n",
        "          )\n",
        "\n",
        "        self.fc1 = Sequential(\n",
        "            Linear(64*6*6*6, out_features=128),\n",
        "            ReLU(),\n",
        "            Dropout(p=0.4)\n",
        "        )\n",
        "        self.class_layer = Linear(128, num_classes)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x).reshape((x.shape[0], -1))\n",
        "        x = self.fc1(x)\n",
        "        class_output = self.class_layer(x)\n",
        "        return class_output\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, std=0.01)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv3d):\n",
        "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
        "\n",
        "        if self.class_layer.bias is not None:\n",
        "            self.class_layer.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwRT9aKClAs_"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN architecture\n",
        "class EXTENDED_ORION_OC(Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = Sequential(\n",
        "          # Definition of Conv1\n",
        "          Conv3d(in_channels=1, out_channels=32, kernel_size=3, stride=2),\n",
        "          BatchNorm3d(num_features=32),\n",
        "          ReLU(),\n",
        "          Dropout3d(p=0.2),\n",
        "\n",
        "          # Definition of Conv2\n",
        "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=64),\n",
        "          ReLU(),\n",
        "          Dropout3d(p=0.3),\n",
        "\n",
        "          # Definition of Conv3\n",
        "          Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=128),\n",
        "          ReLU(),\n",
        "          Dropout3d(p=0.4),\n",
        "\n",
        "          # Definition of Conv4\n",
        "          Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
        "          BatchNorm3d(num_features=256),\n",
        "          ReLU(),\n",
        "          MaxPool3d(kernel_size=2, stride=2),\n",
        "          Dropout3d(p=0.6),\n",
        "          )\n",
        "\n",
        "        self.fc1 = Sequential(\n",
        "            Linear(256*4*4*4, out_features=128),\n",
        "            ReLU(),\n",
        "            Dropout(p=0.4)\n",
        "        )\n",
        "        self.class_layer = Linear(128, num_classes)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x).reshape((x.shape[0], -1))\n",
        "        x = self.fc1(x)\n",
        "        class_output = self.class_layer(x)\n",
        "        return class_output\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, std=0.01)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.Conv3d):\n",
        "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
        "\n",
        "        if self.class_layer.bias is not None:\n",
        "            self.class_layer.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54IMyhKdtrNa"
      },
      "source": [
        "#**Performances evaluation functions**\n",
        "**Accuracies function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XS1JjsIt26X"
      },
      "outputs": [],
      "source": [
        "def accuracy (network, dataloader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    network.to(device)\n",
        "    softmax= torch.nn.Softmax(dim=1)\n",
        "\n",
        "    iterator = tqdm(dataloader, disable=True)\n",
        "    with torch.no_grad():\n",
        "      predicted_class_array = []\n",
        "      true_class_array = []\n",
        "      predicted_pose_array = []\n",
        "      true_pose_array = []\n",
        "\n",
        "      # iterating through the batches\n",
        "      for datas, label_class, label_pose in iterator:\n",
        "        datas = datas.to(device)\n",
        "        label_class = label_class.squeeze().to(device) #remove the dimesions of size 1\n",
        "        label_pose = label_pose.squeeze().to(device)\n",
        "        network.eval()\n",
        "        # forward pass\n",
        "        predicted_class, predicted_pose = network(datas)\n",
        "\n",
        "        # obtain the class from the output\n",
        "        sum_class_pred = torch.sum(predicted_class, dim=0)\n",
        "        predicted_class = torch.argmax(sum_class_pred)\n",
        "        # obtain the pose label from the output\n",
        "        predicted_pose = torch.argmax(predicted_pose, dim=1)\n",
        "        #update the arrays concatenating the results and the true labels\n",
        "        predicted_class_array.append(predicted_class)\n",
        "        true_class_array.append(label_class[0]) #only the first element is important, the others are just repetitions due to the fact that each batch is composed by rotations of the same object\n",
        "        predicted_pose_array.append(predicted_pose)\n",
        "        true_pose_array.append(label_pose) #in this case all the labels are important\n",
        "\n",
        "      #build a single vector\n",
        "      true_class_array= torch.stack(true_class_array)\n",
        "      predicted_class_array= torch.stack(predicted_class_array)\n",
        "      true_pose_array = torch.cat(true_pose_array, axis=0)\n",
        "      predicted_pose_array= torch.cat(predicted_pose_array, dim=0)\n",
        "\n",
        "      # calculate accuracy\n",
        "      class_accuracy = torch.sum(predicted_class_array == true_class_array)/len(true_class_array)\n",
        "      pose_accuracy = torch.sum(predicted_pose_array == true_pose_array)/len(true_pose_array)\n",
        "\n",
        "      #return all the obtained elements\n",
        "      return class_accuracy, pose_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TENX4NNYuhyU"
      },
      "source": [
        "**Losses function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wQRs_1Gt3A-"
      },
      "outputs": [],
      "source": [
        "def loss (network, dataloader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    network.to(device)\n",
        "    # defining the loss functions, creoss-entropy losses are chosen like in the paper\n",
        "    loss_fn_class = CrossEntropyLoss()\n",
        "    loss_fn_pose = CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    iterator = tqdm(dataloader, disable=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      predicted_class_array = []\n",
        "      true_class_array = []\n",
        "      predicted_pose_array = []\n",
        "      true_pose_array = []\n",
        "\n",
        "      # iterating through the batches\n",
        "      for datas, label_class, label_pose in iterator:\n",
        "        datas = datas.to(device)\n",
        "        label_class = label_class.squeeze().to(device) #remove the dimesions of size 1\n",
        "        label_pose = label_pose.squeeze().to(device)\n",
        "        network.eval()\n",
        "        # forward pass\n",
        "        predicted_class, predicted_pose = network(datas)\n",
        "\n",
        "        predicted_class_array.append(predicted_class)\n",
        "        true_class_array.append(label_class) #only the first element is import, the others are just repetions due to the fact that each batch is composed by rotations of the same object\n",
        "        predicted_pose_array.append(predicted_pose)\n",
        "        true_pose_array.append(label_pose) #in this case all the labels are important\n",
        "\n",
        "      #build a single vector\n",
        "      true_class_array= torch.cat(true_class_array, axis=0)\n",
        "      predicted_class_array= torch.cat(predicted_class_array, axis=0)\n",
        "      true_pose_array = torch.cat(true_pose_array, axis=0)\n",
        "      predicted_pose_array = torch.cat(predicted_pose_array, axis=0)\n",
        "\n",
        "      #calculate losses\n",
        "      pose_loss = loss_fn_pose(predicted_pose_array, true_pose_array)\n",
        "      class_loss = loss_fn_class(predicted_class_array, true_class_array)\n",
        "      total_loss = (class_loss + pose_loss)/2 #weighted sum as defined in the paper\n",
        "\n",
        "      #return all the obtained elements\n",
        "      return class_loss, pose_loss, total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzjYU44ynbUa"
      },
      "source": [
        "**Accuracies function for class classificatio only**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RBF1qvXng7R"
      },
      "outputs": [],
      "source": [
        "def accuracy_OC (network, dataloader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    network.to(device)\n",
        "    softmax= torch.nn.Softmax(dim=1)\n",
        "\n",
        "    iterator = tqdm(dataloader, disable=True)\n",
        "    with torch.no_grad():\n",
        "      predicted_class_array = []\n",
        "      true_class_array = []\n",
        "      # iterating through the batches\n",
        "      for datas, label_class, label_pose in iterator:\n",
        "        datas = datas.to(device)\n",
        "        label_class = label_class.squeeze().to(device) #remove the dimesions of size 1\n",
        "        label_pose = label_pose.squeeze().to(device)\n",
        "        network.eval()\n",
        "        # forward pass\n",
        "        predicted_class = network(datas)\n",
        "\n",
        "        # obtain the class from the output\n",
        "        sum_class_pred = torch.sum(predicted_class, dim=0)\n",
        "        predicted_class = torch.argmax(sum_class_pred)\n",
        "        #update the arrays concatenating the results and the true labels\n",
        "        predicted_class_array.append(predicted_class)\n",
        "        true_class_array.append(label_class[0]) #only the first element is important, the others are just repetitions due to the fact that each batch is composed by rotations of the same object\n",
        "\n",
        "      #build a single vector\n",
        "      true_class_array= torch.stack(true_class_array)\n",
        "      predicted_class_array= torch.stack(predicted_class_array)\n",
        "\n",
        "      # calculate accuracy\n",
        "      class_accuracy = torch.sum(predicted_class_array == true_class_array)/len(true_class_array)\n",
        "\n",
        "      #return all the obtained elements\n",
        "      return class_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31wgdtUfnvbe"
      },
      "source": [
        "**Losses function for class classification only**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A3dZbovn2qN"
      },
      "outputs": [],
      "source": [
        "def loss_OC (network, dataloader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    network.to(device)\n",
        "    # defining the loss functions, creoss-entropy losses are chosen like in the paper\n",
        "    loss_fn_class = CrossEntropyLoss()\n",
        "\n",
        "    iterator = tqdm(dataloader, disable=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      predicted_class_array = []\n",
        "      true_class_array = []\n",
        "\n",
        "      # iterating through the batches\n",
        "      for datas, label_class, label_pose in iterator:\n",
        "        datas = datas.to(device)\n",
        "        label_class = label_class.squeeze().to(device) #remove the dimesions of size 1\n",
        "        label_pose = label_pose.squeeze().to(device)\n",
        "        network.eval()\n",
        "        # forward pass\n",
        "        predicted_class = network(datas)\n",
        "\n",
        "        predicted_class_array.append(predicted_class)\n",
        "        true_class_array.append(label_class) #only the first element is import, the others are just repetions due to the fact that each batch is composed by rotations of the same object\n",
        "\n",
        "      #build a single vector\n",
        "      true_class_array= torch.cat(true_class_array, axis=0)\n",
        "      predicted_class_array= torch.cat(predicted_class_array, axis=0)\n",
        "\n",
        "      #calculate losses\n",
        "      class_loss = loss_fn_class(predicted_class_array, true_class_array)\n",
        "\n",
        "      #return all the obtained elements\n",
        "      return class_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMz1NguFNtGm"
      },
      "source": [
        "#**Data logging**\n",
        "**Run this ti deal with class and orientation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "FsmpLwSfNwLN",
        "outputId": "95b35e4b-5913-4bc9-83dd-5b0ead6aeda0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240108_161634-xllgf050</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gigia/Modelnet10_multiple/runs/xllgf050' target=\"_blank\">quiet-wildflower-34</a></strong> to <a href='https://wandb.ai/gigia/Modelnet10_multiple' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gigia/Modelnet10_multiple' target=\"_blank\">https://wandb.ai/gigia/Modelnet10_multiple</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gigia/Modelnet10_multiple/runs/xllgf050' target=\"_blank\">https://wandb.ai/gigia/Modelnet10_multiple/runs/xllgf050</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_metric.Metric at 0x7b233798f400>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# start a new wandb run to track this script\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"Modelnet10_multiple\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"dataset\": \"ModelNet10\",\n",
        "    \"epochs\": 200,\n",
        "    }\n",
        ")\n",
        "wandb.define_metric(\"epoch\")\n",
        "\n",
        "wandb.define_metric(\"validation_loss\", step_metric=\"epoch\")\n",
        "wandb.define_metric(\"class_validation_loss\", step_metric=\"epoch\")\n",
        "wandb.define_metric(\"pose_validation_loss\", step_metric=\"epoch\")\n",
        "wandb.define_metric(\"class_validation_accuracy\", step_metric=\"epoch\")\n",
        "wandb.define_metric(\"pose_validation_accuracy\", step_metric=\"epoch\")\n",
        "wandb.define_metric(\"class_validation_error\", step_metric=\"epoch\")\n",
        "wandb.define_metric(\"pose_validation_error\", step_metric=\"epoch\")\n",
        "#f4c123b14b87239cebd4b2c785e43afba6af6d07"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Glm0Id1Ixfel"
      },
      "source": [
        "**Run this to deal only with class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "9e741fd3e38b409fb2a675edd99ff80e",
            "6dc4a25710f944d1948bd0c5b7d2956c",
            "12ce6ba36725411fab4abb2373675d67",
            "09281af74c4347b085782d3908918483",
            "720483426de64b8c92f1b6e4044ac71e",
            "4923ff7d37de4e3e9fbee03479032e63",
            "2a333b817340492187423b345df0daee",
            "38677bdce5874925bfeafa4df4b1f165"
          ]
        },
        "id": "u81-B3fCxjyL",
        "outputId": "38896ed0-d5f1-4900-c3ac-c6aed6d15936"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:g1yuqdl2) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.011 MB uploaded\\r'), FloatProgress(value=0.15398793565683647, max=1.…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e741fd3e38b409fb2a675edd99ff80e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resilient-flower-28</strong> at: <a href='https://wandb.ai/gigia/Modelnet10_multiple/runs/g1yuqdl2' target=\"_blank\">https://wandb.ai/gigia/Modelnet10_multiple/runs/g1yuqdl2</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240106_170720-g1yuqdl2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:g1yuqdl2). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240106_170724-eyv4k9oy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gigia/Modelnet10_onlyclassesSDG/runs/eyv4k9oy' target=\"_blank\">atomic-wildflower-22</a></strong> to <a href='https://wandb.ai/gigia/Modelnet10_onlyclassesSDG' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gigia/Modelnet10_onlyclassesSDG' target=\"_blank\">https://wandb.ai/gigia/Modelnet10_onlyclassesSDG</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gigia/Modelnet10_onlyclassesSDG/runs/eyv4k9oy' target=\"_blank\">https://wandb.ai/gigia/Modelnet10_onlyclassesSDG/runs/eyv4k9oy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_metric.Metric at 0x79cced7ebf10>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# start a new wandb\n",
        "wandb.init(\n",
        "\n",
        "    project=\"Modelnet10_onlyclassesSDG\",\n",
        "\n",
        "    config={\n",
        "    \"dataset\": \"ModelNet10\",\n",
        "    \"epochs\": 200,\n",
        "    }\n",
        ")\n",
        "wandb.define_metric(\"epoch\")\n",
        "wandb.define_metric(\"class_validation_loss\", step_metric=\"epoch\")\n",
        "wandb.define_metric(\"class_validation_accuracy\", step_metric=\"epoch\")\n",
        "wandb.define_metric(\"class_validation_error\", step_metric=\"epoch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy5R44UNuwKo"
      },
      "source": [
        "# **Training process**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8BG3qy9u1x9"
      },
      "outputs": [],
      "source": [
        "def training(model,train_data_loading, validation_data_loading, epochs, opt, early_stopping):\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "  best_loss=np.inf #set the best value of the loss to infinite\n",
        "  early_stopping_val=0\n",
        "  class_loss_function = CrossEntropyLoss()\n",
        "  pose_loss_function = CrossEntropyLoss()\n",
        "\n",
        "  #start loop for each epoch, defined in input\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    model.train()\n",
        "    elements = tqdm(train_data_loading)\n",
        "    for datas, label_class, label_pose in elements:\n",
        "      #input real labels\n",
        "      datas=datas.to(device)\n",
        "      label_class=label_class.squeeze().to(device)\n",
        "      label_pose=label_pose.squeeze().to(device)\n",
        "      #do the forward pass\n",
        "      label_class_prediction, label_pose_prediction=model(datas)\n",
        "\n",
        "      #calculate losses\n",
        "      class_loss = class_loss_function(label_class_prediction, label_class)\n",
        "      pose_loss = pose_loss_function(label_pose_prediction, label_pose)\n",
        "      train_loss = (class_loss + pose_loss)/2\n",
        "      # logging training losses\n",
        "      wandb.log({\"train_loss\": train_loss, \"pose_train_loss\": pose_loss, \"class_train_loss\": class_loss })\n",
        "\n",
        "      #backword pass\n",
        "      opt.zero_grad()\n",
        "      train_loss.backward()\n",
        "      opt.step()\n",
        "      #view batch results\n",
        "      elements.set_description(f\"Train loss: {train_loss.detach().cpu().numpy()} class loss: {class_loss.detach().cpu().numpy()}  pose loss: {pose_loss.detach().cpu().numpy()}\")\n",
        "    #compute accuracies and losses on validation set\n",
        "    class_accuracy_val, pose_accuracy_val = accuracy(model,validation_data_loading)\n",
        "    class_loss_val, pose_loss_val, total_loss_val=loss(model, validation_data_loading)\n",
        "\n",
        "    # logging validation results\n",
        "    wandb.log({\"epoch\": epoch+1,\n",
        "                \"validation_loss\": total_loss_val,\n",
        "                \"pose_validation_loss\": pose_loss_val,\n",
        "                \"class_validation_loss\": class_loss_val,\n",
        "                \"class_validation_accuracy\": class_accuracy_val,\n",
        "                \"pose_validation_accuracy\": pose_accuracy_val,\n",
        "                \"class_validation_error\": 1-class_accuracy_val,\n",
        "                \"pose_validation_error\": 1-pose_accuracy_val})\n",
        "\n",
        "    #display elements\n",
        "    print(f\"class accuracy: {class_accuracy_val}, \\n pose accuracy: {pose_accuracy_val}\")\n",
        "    print(f\"class loss: {class_loss_val}, \\n pose loss: {pose_loss} , \\n total loss: {total_loss_val}\")\n",
        "    #updating model looking at the loss\n",
        "    if total_loss_val < best_loss:\n",
        "        print(\"Saved Model\")\n",
        "        #print(model)\n",
        "        torch.save(model.state_dict(), \"model.pt\")\n",
        "        best_loss = total_loss_val\n",
        "        early_stopping_val=0 #re-initialize the count\n",
        "    else:\n",
        "        early_stopping_val+=1\n",
        "        #check early stopping and exit training loop\n",
        "    if early_stopping_val >= early_stopping :\n",
        "      print(\"EARLY STOPPING\")\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-MjlcZOmnfp"
      },
      "source": [
        "**Define a training process for the usage of calsses only, without considering the orientation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJAddDoUml2q"
      },
      "outputs": [],
      "source": [
        "def training_OC(model,train_data_loading, validation_data_loading, epochs, opt, early_stopping):\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "  best_loss=np.inf #set the best value of the loss to infinite\n",
        "  early_stopping_val=0\n",
        "  class_loss_function = CrossEntropyLoss()\n",
        "\n",
        "  #start loop for each epoch, defined in input\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    model.train()\n",
        "    elements = tqdm(train_data_loading)\n",
        "    for datas, label_class, label_pose in elements:\n",
        "      #input real labels\n",
        "      datas=datas.to(device)\n",
        "      label_class=label_class.squeeze().to(device)\n",
        "      label_pose=label_pose.squeeze().to(device)\n",
        "      #do the forward pass\n",
        "      label_class_prediction=model(datas)\n",
        "\n",
        "      #calculate losses\n",
        "      class_loss = class_loss_function(label_class_prediction, label_class)\n",
        "\n",
        "\n",
        "      #backword pass\n",
        "      opt.zero_grad()\n",
        "      class_loss.backward()\n",
        "      opt.step()\n",
        "      #view batch results\n",
        "      elements.set_description(f\"class loss: {class_loss.detach().cpu().numpy()}\")\n",
        "    #compute accuracies and losses on validation set\n",
        "    class_accuracy_val = accuracy_OC(model,validation_data_loading)\n",
        "    class_loss_val = loss_OC(model, validation_data_loading)\n",
        "    # logging validation results\n",
        "    wandb.log({\"epoch\": epoch+1,\n",
        "                \"class_validation_loss\": class_loss_val,\n",
        "                \"class_validation_accuracy\": class_accuracy_val,\n",
        "                \"class_validation_error\": 1-class_accuracy_val})\n",
        "\n",
        "    #display elements\n",
        "    print(f\"class accuracy: {class_accuracy_val}\")\n",
        "    print(f\"class loss: {class_loss_val}\")\n",
        "    #updating model looking at the loss\n",
        "    if class_loss_val < best_loss:\n",
        "        print(\"Saved Model\")\n",
        "        torch.save(model.state_dict(), \"model.pt\")\n",
        "        best_loss = class_loss_val\n",
        "        early_stopping_val=0 #re-initialize the count\n",
        "    else:\n",
        "        early_stopping_val+=1\n",
        "        #check early stopping and exit training loop\n",
        "    if early_stopping_val >= early_stopping :\n",
        "      print(\"EARLY STOPPING\")\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0IDf9iOvykr"
      },
      "source": [
        "# **Initialization and running**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bb9afadecd6a42f59b4a55dce28a2cec",
            "4245dde891394d0884c1e0a3ecef4131",
            "d39bb2b74a394290bffd8433cc8560b9",
            "77acc982ed0b4e5f9f51fb26344e9ea0",
            "c9eda2efe6204727b2781d746a25f909",
            "ff2c9b878f05407791d750acdb8de14f",
            "0d61094c290042a28e2508d8aed997f9",
            "89e8862130d242a4848f48c771a64633"
          ]
        },
        "id": "81j1m4N8vw14",
        "outputId": "250f330f-9f43-44f1-ca99-0d3b778fc8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.7462725639343262 class loss: 0.23589569330215454  pose loss: 1.2566494941711426: 100%|██████████| 2742/2742 [03:43<00:00, 12.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.8833780288696289, \n",
            " pose accuracy: 0.6513069868087769\n",
            "class loss: 0.41891226172447205, \n",
            " pose loss: 1.2566494941711426 , \n",
            " total loss: 0.9178043007850647\n",
            "Saved Model\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.7148725986480713 class loss: 0.27907687425613403  pose loss: 1.1506683826446533: 100%|██████████| 2742/2742 [03:47<00:00, 12.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9075067043304443, \n",
            " pose accuracy: 0.7408958673477173\n",
            "class loss: 0.3484748899936676, \n",
            " pose loss: 1.1506683826446533 , \n",
            " total loss: 0.6734451651573181\n",
            "Saved Model\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.45594102144241333 class loss: 0.18439263105392456  pose loss: 0.7274894118309021: 100%|██████████| 2742/2742 [03:47<00:00, 12.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9088472127914429, \n",
            " pose accuracy: 0.7723413705825806\n",
            "class loss: 0.34105372428894043, \n",
            " pose loss: 0.7274894118309021 , \n",
            " total loss: 0.5921032428741455\n",
            "Saved Model\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.4412118196487427 class loss: 0.19554471969604492  pose loss: 0.6868789196014404: 100%|██████████| 2742/2742 [03:47<00:00, 12.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9101877212524414, \n",
            " pose accuracy: 0.7846291661262512\n",
            "class loss: 0.3275062143802643, \n",
            " pose loss: 0.6868789196014404 , \n",
            " total loss: 0.5486470460891724\n",
            "Saved Model\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.5359147787094116 class loss: 0.2710840106010437  pose loss: 0.8007455468177795: 100%|██████████| 2742/2742 [03:47<00:00, 12.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9195711016654968, \n",
            " pose accuracy: 0.7952971458435059\n",
            "class loss: 0.31359121203422546, \n",
            " pose loss: 0.8007455468177795 , \n",
            " total loss: 0.5262632369995117\n",
            "Saved Model\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 1.200736403465271 class loss: 1.008948802947998  pose loss: 1.392524003982544: 100%|██████████| 2742/2742 [03:48<00:00, 12.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9101877212524414, \n",
            " pose accuracy: 0.8007707595825195\n",
            "class loss: 0.31739455461502075, \n",
            " pose loss: 1.392524003982544 , \n",
            " total loss: 0.5112171173095703\n",
            "Saved Model\n",
            "Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.3519889712333679 class loss: 0.17114736139774323  pose loss: 0.5328305959701538: 100%|██████████| 2742/2742 [03:47<00:00, 12.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9235925078392029, \n",
            " pose accuracy: 0.8038427233695984\n",
            "class loss: 0.3048090636730194, \n",
            " pose loss: 0.5328305959701538 , \n",
            " total loss: 0.49486207962036133\n",
            "Saved Model\n",
            "Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.1884714663028717 class loss: 0.04515624791383743  pose loss: 0.3317866921424866: 100%|██████████| 2742/2742 [03:47<00:00, 12.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9222520589828491, \n",
            " pose accuracy: 0.8038427233695984\n",
            "class loss: 0.3125905394554138, \n",
            " pose loss: 0.3317866921424866 , \n",
            " total loss: 0.5004070401191711\n",
            "Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.4450102746486664 class loss: 0.060354314744472504  pose loss: 0.829666256904602: 100%|██████████| 2742/2742 [03:47<00:00, 12.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9088472127914429, \n",
            " pose accuracy: 0.8087578415870667\n",
            "class loss: 0.32397139072418213, \n",
            " pose loss: 0.829666256904602 , \n",
            " total loss: 0.5030349493026733\n",
            "Epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.3780441880226135 class loss: 0.06333021819591522  pose loss: 0.6927581429481506: 100%|██████████| 2742/2742 [03:47<00:00, 12.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9075067043304443, \n",
            " pose accuracy: 0.8039544224739075\n",
            "class loss: 0.3213520050048828, \n",
            " pose loss: 0.6927581429481506 , \n",
            " total loss: 0.4992692172527313\n",
            "Epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.4247141480445862 class loss: 0.1399000883102417  pose loss: 0.7095282077789307: 100%|██████████| 2742/2742 [03:47<00:00, 12.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.915549635887146, \n",
            " pose accuracy: 0.8106568455696106\n",
            "class loss: 0.31171268224716187, \n",
            " pose loss: 0.7095282077789307 , \n",
            " total loss: 0.49077939987182617\n",
            "Saved Model\n",
            "Epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.28953051567077637 class loss: 0.2816294729709625  pose loss: 0.2974315285682678: 100%|██████████| 2742/2742 [03:47<00:00, 12.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.904825747013092, \n",
            " pose accuracy: 0.8080317378044128\n",
            "class loss: 0.33646252751350403, \n",
            " pose loss: 0.2974315285682678 , \n",
            " total loss: 0.5128036141395569\n",
            "Epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.3524268865585327 class loss: 0.15567731857299805  pose loss: 0.5491764545440674: 100%|██████████| 2742/2742 [03:47<00:00, 12.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9182305932044983, \n",
            " pose accuracy: 0.8119973540306091\n",
            "class loss: 0.3450016975402832, \n",
            " pose loss: 0.5491764545440674 , \n",
            " total loss: 0.5187575221061707\n",
            "Epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.4217258095741272 class loss: 0.14199110865592957  pose loss: 0.7014604806900024: 100%|██████████| 2742/2742 [03:47<00:00, 12.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9101877212524414, \n",
            " pose accuracy: 0.8094839453697205\n",
            "class loss: 0.34094855189323425, \n",
            " pose loss: 0.7014604806900024 , \n",
            " total loss: 0.5121887922286987\n",
            "Epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.2383001446723938 class loss: 0.1235443651676178  pose loss: 0.3530559241771698: 100%|██████████| 2742/2742 [03:46<00:00, 12.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9235925078392029, \n",
            " pose accuracy: 0.8149017095565796\n",
            "class loss: 0.32108086347579956, \n",
            " pose loss: 0.3530559241771698 , \n",
            " total loss: 0.4953879714012146\n",
            "Epoch: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.2369711995124817 class loss: 0.06403380632400513  pose loss: 0.40990859270095825: 100%|██████████| 2742/2742 [03:47<00:00, 12.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.915549635887146, \n",
            " pose accuracy: 0.8165773153305054\n",
            "class loss: 0.34792420268058777, \n",
            " pose loss: 0.40990859270095825 , \n",
            " total loss: 0.5189892649650574\n",
            "Epoch: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.21255242824554443 class loss: 0.024653170257806778  pose loss: 0.4004516899585724: 100%|██████████| 2742/2742 [03:46<00:00, 12.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9209115505218506, \n",
            " pose accuracy: 0.8169124126434326\n",
            "class loss: 0.3408443033695221, \n",
            " pose loss: 0.4004516899585724 , \n",
            " total loss: 0.5108444094657898\n",
            "Epoch: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.26973217725753784 class loss: 0.14087575674057007  pose loss: 0.3985885977745056: 100%|██████████| 2742/2742 [03:47<00:00, 12.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.915549635887146, \n",
            " pose accuracy: 0.8150692582130432\n",
            "class loss: 0.34467676281929016, \n",
            " pose loss: 0.3985885977745056 , \n",
            " total loss: 0.5160363912582397\n",
            "Epoch: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.3269103765487671 class loss: 0.1842157542705536  pose loss: 0.469605028629303: 100%|██████████| 2742/2742 [03:47<00:00, 12.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9182305932044983, \n",
            " pose accuracy: 0.8168565630912781\n",
            "class loss: 0.34037989377975464, \n",
            " pose loss: 0.469605028629303 , \n",
            " total loss: 0.5071958303451538\n",
            "Epoch: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.1647462248802185 class loss: 0.01096868235617876  pose loss: 0.3185237646102905: 100%|██████████| 2742/2742 [03:46<00:00, 12.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9128686785697937, \n",
            " pose accuracy: 0.8174709677696228\n",
            "class loss: 0.33765166997909546, \n",
            " pose loss: 0.3185237646102905 , \n",
            " total loss: 0.5039970874786377\n",
            "Epoch: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.4671337306499481 class loss: 0.11024781316518784  pose loss: 0.8240196704864502: 100%|██████████| 2742/2742 [03:47<00:00, 12.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.915549635887146, \n",
            " pose accuracy: 0.8166331648826599\n",
            "class loss: 0.34796154499053955, \n",
            " pose loss: 0.8240196704864502 , \n",
            " total loss: 0.5193832516670227\n",
            "Epoch: 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.10183215886354446 class loss: 0.0050135101191699505  pose loss: 0.1986508071422577: 100%|██████████| 2742/2742 [03:47<00:00, 12.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9168900847434998, \n",
            " pose accuracy: 0.8181970715522766\n",
            "class loss: 0.337248295545578, \n",
            " pose loss: 0.1986508071422577 , \n",
            " total loss: 0.5073989629745483\n",
            "Epoch: 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.20023521780967712 class loss: 0.06040205433964729  pose loss: 0.34006837010383606: 100%|██████████| 2742/2742 [03:47<00:00, 12.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9182305932044983, \n",
            " pose accuracy: 0.8184204697608948\n",
            "class loss: 0.3529461920261383, \n",
            " pose loss: 0.34006837010383606 , \n",
            " total loss: 0.5220812559127808\n",
            "Epoch: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.4236655831336975 class loss: 0.27063748240470886  pose loss: 0.5766937136650085: 100%|██████████| 2742/2742 [03:47<00:00, 12.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9195711016654968, \n",
            " pose accuracy: 0.818755567073822\n",
            "class loss: 0.3488357663154602, \n",
            " pose loss: 0.5766937136650085 , \n",
            " total loss: 0.5164237022399902\n",
            "Epoch: 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.42111507058143616 class loss: 0.2368006706237793  pose loss: 0.605429470539093: 100%|██████████| 2742/2742 [03:47<00:00, 12.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9115281701087952, \n",
            " pose accuracy: 0.8177502155303955\n",
            "class loss: 0.3702520728111267, \n",
            " pose loss: 0.605429470539093 , \n",
            " total loss: 0.5352363586425781\n",
            "Epoch: 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.18774127960205078 class loss: 0.1109754666686058  pose loss: 0.26450708508491516: 100%|██████████| 2742/2742 [03:46<00:00, 12.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9182305932044983, \n",
            " pose accuracy: 0.8234472870826721\n",
            "class loss: 0.36179932951927185, \n",
            " pose loss: 0.26450708508491516 , \n",
            " total loss: 0.5237656235694885\n",
            "Epoch: 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.23438574373722076 class loss: 0.12031985074281693  pose loss: 0.3484516441822052: 100%|██████████| 2742/2742 [03:47<00:00, 12.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9249330163002014, \n",
            " pose accuracy: 0.8230562806129456\n",
            "class loss: 0.3638957440853119, \n",
            " pose loss: 0.3484516441822052 , \n",
            " total loss: 0.527090847492218\n",
            "Epoch: 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.36163318157196045 class loss: 0.11711463332176208  pose loss: 0.6061517596244812: 100%|██████████| 2742/2742 [03:46<00:00, 12.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9209115505218506, \n",
            " pose accuracy: 0.8193699717521667\n",
            "class loss: 0.36203062534332275, \n",
            " pose loss: 0.6061517596244812 , \n",
            " total loss: 0.5305024981498718\n",
            "Epoch: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.27365660667419434 class loss: 0.13407187163829803  pose loss: 0.41324135661125183: 100%|██████████| 2742/2742 [03:46<00:00, 12.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9128686785697937, \n",
            " pose accuracy: 0.820040225982666\n",
            "class loss: 0.3768634498119354, \n",
            " pose loss: 0.41324135661125183 , \n",
            " total loss: 0.5417088270187378\n",
            "Epoch: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.18905864655971527 class loss: 0.0980837419629097  pose loss: 0.28003355860710144: 100%|██████████| 2742/2742 [03:46<00:00, 12.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9142091274261475, \n",
            " pose accuracy: 0.8239499926567078\n",
            "class loss: 0.36500391364097595, \n",
            " pose loss: 0.28003355860710144 , \n",
            " total loss: 0.5272852778434753\n",
            "Epoch: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train loss: 0.08810223639011383 class loss: 0.02677772380411625  pose loss: 0.14942674338817596: 100%|██████████| 2742/2742 [03:47<00:00, 12.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class accuracy: 0.9195711016654968, \n",
            " pose accuracy: 0.8213807344436646\n",
            "class loss: 0.36984366178512573, \n",
            " pose loss: 0.14942674338817596 , \n",
            " total loss: 0.5329952239990234\n",
            "EARLY STOPPING\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb9afadecd6a42f59b4a55dce28a2cec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>class_train_loss</td><td>▆█▆▃▆▃▆▂▂▄▄▅▂▁▂▂▁▂▃▂▃▁▂▂▂▂▂▁▁▂▁▂▁▃▁▁▁▂▃▁</td></tr><tr><td>class_validation_accuracy</td><td>▁▅▅▆▇▆██▅▅▆▅▇▆█▆▇▆▇▆▆▇▇▇▆▇█▇▆▆▇</td></tr><tr><td>class_validation_error</td><td>█▄▄▃▂▃▁▁▄▄▃▄▂▃▁▃▂▃▂▃▃▂▂▂▃▂▁▂▃▃▂</td></tr><tr><td>class_validation_loss</td><td>█▄▃▂▂▂▁▁▂▂▁▃▃▃▂▄▃▃▃▃▄▃▄▄▅▄▅▅▅▅▅</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>pose_train_loss</td><td>█▄▄▃▄▂▄▂▃▃▄▃▁▁▂▂▁▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▁▂▂▁▂▁▂▂</td></tr><tr><td>pose_validation_accuracy</td><td>▁▅▆▆▇▇▇▇▇▇▇▇█▇█████████████████</td></tr><tr><td>pose_validation_error</td><td>█▄▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>pose_validation_loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▅▃▄▂▄▂▃▃▄▃▁▁▂▂▁▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▃▂▁▂▁▂▂</td></tr><tr><td>validation_loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>class_train_loss</td><td>0.02678</td></tr><tr><td>class_validation_accuracy</td><td>0.91957</td></tr><tr><td>class_validation_error</td><td>0.08043</td></tr><tr><td>class_validation_loss</td><td>0.36984</td></tr><tr><td>epoch</td><td>31</td></tr><tr><td>pose_train_loss</td><td>0.14943</td></tr><tr><td>pose_validation_accuracy</td><td>0.82138</td></tr><tr><td>pose_validation_error</td><td>0.17862</td></tr><tr><td>pose_validation_loss</td><td>0.69615</td></tr><tr><td>train_loss</td><td>0.0881</td></tr><tr><td>validation_loss</td><td>0.533</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">quiet-wildflower-34</strong> at: <a href='https://wandb.ai/gigia/Modelnet10_multiple/runs/xllgf050' target=\"_blank\">https://wandb.ai/gigia/Modelnet10_multiple/runs/xllgf050</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240108_161634-xllgf050/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from torch.optim import SGD, Adam\n",
        "\n",
        "#manually define the architecture type\n",
        "chosen_model=2;\n",
        "#manually define the optimizaer type\n",
        "chosen_opt=2;\n",
        "\n",
        "#define epochs and early stopping\n",
        "epochs=200\n",
        "es=20\n",
        "\n",
        "if chosen_model==1 and chosen_opt==1:\n",
        "  model=ORION(num_classes, num_poses)\n",
        "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
        "  training(model, train_dataloader, validation_dataloader, epochs, opt, es) #91.5% sul test, 96% sul train\n",
        "  #lr=0.001\n",
        "  #momentum=0.9: pose acc 0.83(validation) 0.825(test)\n",
        "  #momentum=0: pose acc 0.8044(validation) 0.8177(test)\n",
        "  #momentum=0.4: pose acc 0.8011(validation) 0.8105(test)\n",
        "elif chosen_model==1 and chosen_opt==2:\n",
        "  model=ORION(num_classes, num_poses)\n",
        "  opt = Adam(model.parameters(), lr=1e-4)\n",
        "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "elif chosen_model==2 and chosen_opt==1:\n",
        "  model=EXTENDED_ORION(num_classes, num_poses)\n",
        "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True) #teoricamente va\n",
        "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "elif chosen_model==2 and chosen_opt==2:\n",
        "  model=EXTENDED_ORION(num_classes, num_poses)\n",
        "  opt = Adam(model.parameters(), lr=1e-4) #teoricamente va\n",
        "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "\n",
        "#Leaky Relu\n",
        "elif chosen_model==3 and chosen_opt==1:\n",
        "  model=ORION_LRELU(num_classes, num_poses)\n",
        "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
        "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "elif chosen_model==3 and chosen_opt==2:\n",
        "  model=ORION_LRELU(num_classes, num_poses)\n",
        "  opt = Adam(model.parameters(), lr=1e-4)\n",
        "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "elif chosen_model==4 and chosen_opt==1:\n",
        "  model=EXTENDED_ORION_LRELU(num_classes, num_poses)\n",
        "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
        "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "elif chosen_model==4 and chosen_opt==2:\n",
        "  model=EXTENDED_ORION_LRELU(num_classes, num_poses)\n",
        "  opt = Adam(model.parameters(), lr=1e-4)\n",
        "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "\n",
        "#ELU\n",
        "elif chosen_model==5 and chosen_opt==1:\n",
        "  model=ORION_ELU(num_classes, num_poses)\n",
        "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
        "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "elif chosen_model==5 and chosen_opt==2:\n",
        "  model=ORION_ELU(num_classes, num_poses)\n",
        "  opt = Adam(model.parameters(), lr=1e-4)\n",
        "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "elif chosen_model==6 and chosen_opt==1:\n",
        "  model=EXTENDED_ORION_ELU(num_classes, num_poses)\n",
        "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
        "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "elif chosen_model==6 and chosen_opt==2:\n",
        "  model=EXTENDED_ORION_ELU(num_classes, num_poses)\n",
        "  opt = Adam(model.parameters(), lr=1e-4)\n",
        "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "\n",
        "#Only calsses evalyuation\n",
        "elif chosen_model==7 and chosen_opt==1:\n",
        "  model=ORION_OC(num_classes)\n",
        "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True) #validation accuracy 94.67% test 90.75%\n",
        "  training_OC(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "elif chosen_model==7 and chosen_opt==2:\n",
        "  model=ORION_OC(num_classes)\n",
        "  opt = Adam(model.parameters(), lr=1e-4) #validation accuracy 93.33% test 90.86%\n",
        "  training_OC(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "elif chosen_model==8 and chosen_opt==1:\n",
        "  model=EXTENDED_ORION_OC(num_classes)\n",
        "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
        "  training_OC(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "elif chosen_model==8 and chosen_opt==2:\n",
        "  model=EXTENDED_ORION_OC(num_classes)\n",
        "  opt = Adam(model.parameters(), lr=1e-4)\n",
        "  training_OC(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
        "\n",
        "\n",
        "\n",
        "else:\n",
        "  print(\"Indeces are wrong\")\n",
        "\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUYP_76OyGz9"
      },
      "source": [
        "# **Testing Phase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8jzoQBoyOS-",
        "outputId": "9427d689-40e1-4c22-bf34-c64a2da1154e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Class Accuracy : tensor(0.9155, device='cuda:0')\n",
            "Validation Pose Accuracy : tensor(0.8107, device='cuda:0')\n",
            "Test Class Accuracy : tensor(0.9335, device='cuda:0')\n",
            "Test Pose Accuracy : tensor(0.8259, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"model.pt\"))\n",
        "\n",
        "val_class_accuracy, val_pose_accuracy = accuracy(model, validation_dataloader)\n",
        "#val_class_accuracy = accuracy_OC(model, validation_dataloader)\n",
        "\n",
        "print(\"Validation Class Accuracy : \"+str(val_class_accuracy))\n",
        "print(\"Validation Pose Accuracy : \"+str(val_pose_accuracy))\n",
        "\n",
        "test_class_accuracy, test_pose_accuracy = accuracy(model, test_dataloader)\n",
        "#test_class_accuracy = accuracy_OC(model, test_dataloader)\n",
        "\n",
        "\n",
        "print(\"Test Class Accuracy : \"+str(test_class_accuracy))\n",
        "print(\"Test Pose Accuracy : \"+str(test_pose_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCxKTzVOAE0g"
      },
      "source": [
        "# **Print mismatched objects**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFzfqhO8AJPM"
      },
      "outputs": [],
      "source": [
        "#!pip install h5py numpy matplotlib ipyvolume\n",
        "#!pip install h5py matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import ipyvolume as ipv\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model=ORION(10,105)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "iterator = tqdm(test_dataloader, disable=True)\n",
        "for datas, label_class, label_pose in iterator:\n",
        "      datas = datas.to(device)\n",
        "      label_class = label_class.squeeze().to(device) #remove the dimesions of size 1\n",
        "      label_pose = label_pose.squeeze().to(device)\n",
        "\n",
        "      predicted_class, predicted_pose=model(datas)\n",
        "      sum_class_pred = torch.sum(predicted_class, dim=0)\n",
        "      predicted_class = torch.argmax(sum_class_pred).item()\n",
        "\n",
        "      for index in range(datas.size(0)):\n",
        "        if predicted_class == label_class[index].item():\n",
        "          print(datas.size())\n",
        "          datass = datas[index].squeeze(0).cpu().detach().numpy()\n",
        "          print(\"TRUE CLASS\")\n",
        "          print(label_class[index].item())\n",
        "\n",
        "\n",
        "          # Create a 3D plot\n",
        "          fig = plt.figure()\n",
        "          ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "          # Create a meshgrid for the 3D data\n",
        "          x, y, z = np.meshgrid(range(32), range(32), range(32))\n",
        "          x, y, z = x.flatten(), y.flatten(), z.flatten()\n",
        "          data_flat = datass.flatten()\n",
        "\n",
        "          # Filter coordinates and data where the value is equal to 1\n",
        "          x_filtered = z[data_flat == 1]\n",
        "          y_filtered = x[data_flat == 1]\n",
        "          z_filtered = y[data_flat == 1]\n",
        "\n",
        "          # Plot the filtered points\n",
        "          ax.scatter(x_filtered, y_filtered, z_filtered, c='blue', marker='o')\n",
        "\n",
        "          # Customize the plot as needed\n",
        "          ax.set_xlabel('X-axis')\n",
        "          ax.set_ylabel('Y-axis')\n",
        "          ax.set_zlabel('Z-axis')\n",
        "          ax.set_title('3D Scatter Plot of HDF5 Data')\n",
        "\n",
        "          # Show the plot\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzWhxppSe3Bh"
      },
      "source": [
        "**Show network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBS2iiA5dTxw"
      },
      "outputs": [],
      "source": [
        "#!pip install torchkeras\n",
        "#!pip install --upgrade torchkeras\n",
        "\n",
        "#!pip install torchviz graphviz\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchviz import make_dot\n",
        "\n",
        "# Assuming ORION is your PyTorch model\n",
        "model = ORION(num_classes, num_poses)\n",
        "\n",
        "# Dummy input for visualization\n",
        "dummy_input = Variable(torch.randn(1, 1, 32 , 32, 32))  # You may need to adjust the input size\n",
        "\n",
        "# Generate a dynamic graph of the model\n",
        "dot = make_dot(model(dummy_input), params=dict(model.named_parameters()))\n",
        "\n",
        "# Display the graph directly in Colab\n",
        "dot.render('/tmp/model_1', format='png', cleanup=True)\n",
        "dot.render('/tmp/model_1', format='png', cleanup=True)\n",
        "\n",
        "from IPython.display import Image\n",
        "Image('/tmp/model_1.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHciGoUhjKEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394affdf-998f-4a64-e753-1d1d5f7d8cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv3d-1       [-1, 32, 15, 15, 15]             896\n",
            "       BatchNorm3d-2       [-1, 32, 15, 15, 15]              64\n",
            "              ReLU-3       [-1, 32, 15, 15, 15]               0\n",
            "         Dropout3d-4       [-1, 32, 15, 15, 15]               0\n",
            "            Conv3d-5       [-1, 64, 13, 13, 13]          55,360\n",
            "       BatchNorm3d-6       [-1, 64, 13, 13, 13]             128\n",
            "              ReLU-7       [-1, 64, 13, 13, 13]               0\n",
            "         Dropout3d-8       [-1, 64, 13, 13, 13]               0\n",
            "            Conv3d-9      [-1, 128, 11, 11, 11]         221,312\n",
            "      BatchNorm3d-10      [-1, 128, 11, 11, 11]             256\n",
            "             ReLU-11      [-1, 128, 11, 11, 11]               0\n",
            "        Dropout3d-12      [-1, 128, 11, 11, 11]               0\n",
            "           Conv3d-13         [-1, 256, 9, 9, 9]         884,992\n",
            "      BatchNorm3d-14         [-1, 256, 9, 9, 9]             512\n",
            "             ReLU-15         [-1, 256, 9, 9, 9]               0\n",
            "        MaxPool3d-16         [-1, 256, 4, 4, 4]               0\n",
            "        Dropout3d-17         [-1, 256, 4, 4, 4]               0\n",
            "           Linear-18                  [-1, 128]       2,097,280\n",
            "             ReLU-19                  [-1, 128]               0\n",
            "          Dropout-20                  [-1, 128]               0\n",
            "           Linear-21                   [-1, 10]           1,290\n",
            "           Linear-22                  [-1, 105]          13,545\n",
            "================================================================\n",
            "Total params: 3,275,635\n",
            "Trainable params: 3,275,635\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.12\n",
            "Forward/backward pass size (MB): 17.31\n",
            "Params size (MB): 12.50\n",
            "Estimated Total Size (MB): 29.93\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#!pip install torchsummary\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "# Create an instance of the model\n",
        "model = EXTENDED_ORION(num_classes=10, num_poses=105)\n",
        "\n",
        "# Print the summary to see the block scheme representation\n",
        "summary(model, (1, 32, 32, 32), device=\"cpu\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "54IMyhKdtrNa"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e741fd3e38b409fb2a675edd99ff80e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6dc4a25710f944d1948bd0c5b7d2956c",
              "IPY_MODEL_12ce6ba36725411fab4abb2373675d67"
            ],
            "layout": "IPY_MODEL_09281af74c4347b085782d3908918483"
          }
        },
        "6dc4a25710f944d1948bd0c5b7d2956c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_720483426de64b8c92f1b6e4044ac71e",
            "placeholder": "​",
            "style": "IPY_MODEL_4923ff7d37de4e3e9fbee03479032e63",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "12ce6ba36725411fab4abb2373675d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a333b817340492187423b345df0daee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38677bdce5874925bfeafa4df4b1f165",
            "value": 1
          }
        },
        "09281af74c4347b085782d3908918483": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "720483426de64b8c92f1b6e4044ac71e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4923ff7d37de4e3e9fbee03479032e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a333b817340492187423b345df0daee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38677bdce5874925bfeafa4df4b1f165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb9afadecd6a42f59b4a55dce28a2cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4245dde891394d0884c1e0a3ecef4131",
              "IPY_MODEL_d39bb2b74a394290bffd8433cc8560b9"
            ],
            "layout": "IPY_MODEL_77acc982ed0b4e5f9f51fb26344e9ea0"
          }
        },
        "4245dde891394d0884c1e0a3ecef4131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9eda2efe6204727b2781d746a25f909",
            "placeholder": "​",
            "style": "IPY_MODEL_ff2c9b878f05407791d750acdb8de14f",
            "value": "0.026 MB of 0.026 MB uploaded\r"
          }
        },
        "d39bb2b74a394290bffd8433cc8560b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d61094c290042a28e2508d8aed997f9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89e8862130d242a4848f48c771a64633",
            "value": 1
          }
        },
        "77acc982ed0b4e5f9f51fb26344e9ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9eda2efe6204727b2781d746a25f909": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff2c9b878f05407791d750acdb8de14f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d61094c290042a28e2508d8aed997f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89e8862130d242a4848f48c771a64633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}