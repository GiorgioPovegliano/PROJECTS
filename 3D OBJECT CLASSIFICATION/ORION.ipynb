{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYU0SIRjqfgb"
   },
   "source": [
    "# **Classification of the ModelNet dataset with ORION**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJ0S5s-yr0R4"
   },
   "source": [
    "**Installation of the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bd8J71-Iqi_t",
    "outputId": "8e0d8182-f807-4774-dab1-025c63bf907c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu121)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.41 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.39.2 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.2\n"
     ]
    }
   ],
   "source": [
    "#connect to Google drive to access datasets\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "!pip install torchvision\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "#install wandb in order to obtain graphs\n",
    "!pip install wandb\n",
    "import wandb\n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8pa0Izfr5mT"
   },
   "source": [
    "\n",
    "#**Data pre-processing**\n",
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n-t8ezbfrA8L"
   },
   "outputs": [],
   "source": [
    "class ImportDatas(Dataset):\n",
    "  def __init__(self, file_txt_path, transform=None):\n",
    "\n",
    "        with open(file_txt_path) as file:\n",
    "            lines = file.readlines()\n",
    "        #path to import the dataset\n",
    "        self.paths = [r\"/content/drive/MyDrive/ORION_dataset/ModelNet10_12rot/\" + line.strip('\\n')[40:] for line in lines]\n",
    "        self.data, self.label, self.label_pose = self._combine_files()\n",
    "        self.transform = transform\n",
    "\n",
    "  def _combine_files(self):\n",
    "    #extract objects and labels\n",
    "        combined_data = None\n",
    "        combined_label = None\n",
    "        combined_label_pose = None\n",
    "\n",
    "        for path in self.paths:\n",
    "            file = h5py.File(path, 'r')\n",
    "            data = file['data']\n",
    "\n",
    "            label = np.int64(file['label'])\n",
    "            label_pose = np.int64(file['label_pose'])\n",
    "\n",
    "            if combined_data is None:\n",
    "                combined_data = data\n",
    "                combined_label = label\n",
    "                combined_label_pose = label_pose\n",
    "            else:\n",
    "                combined_data = np.concatenate((combined_data, data), axis=0)\n",
    "                combined_label = np.concatenate((combined_label, label), axis=0)\n",
    "                combined_label_pose = np.concatenate((combined_label_pose, label_pose), axis=0)\n",
    "\n",
    "        return combined_data, combined_label, combined_label_pose\n",
    "\n",
    "  def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        label = self.label[idx]\n",
    "        label_pose = self.label_pose[idx]\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        return data, label, label_pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Xhw8vJOTrdV_"
   },
   "outputs": [],
   "source": [
    "class CropTransform(torch.nn.Module):\n",
    "  def forward(self, datas):\n",
    "    #cropping datas from [36x36x36] to [32x32x32]\n",
    "    cropped_datas = datas[: ,2:34, 2:34, 2:34]\n",
    "    return cropped_datas\n",
    "\n",
    "class ToTensor(torch.nn.Module):\n",
    "  def forward(self, datas):\n",
    "    tensor = torch.from_numpy(datas.astype(np.float32)) #transform datas into tensor\n",
    "    return tensor\n",
    "#composition of the transformation to apply\n",
    "transforms = Compose([\n",
    "    CropTransform(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wot6FNfsEeJ"
   },
   "source": [
    "**Datas uploading from Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zaH-mnukrvD2"
   },
   "outputs": [],
   "source": [
    "#define the type of set poseplan, those values are the used inside the ORION architecture\n",
    "num_classes=10\n",
    "num_poses=105\n",
    "\n",
    "\n",
    "# initialize paths\n",
    "train_dataset = ImportDatas(r'/content/drive/MyDrive/ORION_dataset/ModelNet10_12rot/poseplan_MN10_12/hdf5/train_allrot/train.hdf5.txt', transform=transforms)\n",
    "validation_dataset = ImportDatas(r'/content/drive/MyDrive/ORION_dataset/ModelNet10_12rot/poseplan_MN10_12/hdf5/validation_allrot/train.hdf5.txt', transform=transforms)\n",
    "test_dataset = ImportDatas(r'/content/drive/MyDrive/ORION_dataset/ModelNet10_12rot/poseplan_MN10_12/hdf5/test_allrot/train.hdf5.txt', transform=transforms)\n",
    "\n",
    "# load datas\n",
    "#change batch size for validation and test. 12 if dealing with 12 rotations, 24 for 24 rotations\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=12, shuffle=False, num_workers=2, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmuqMUL0sxQ_"
   },
   "source": [
    "#**Print one object with label**\n",
    "We used this code in order to visualize the dataset composition. It's not necessary to train and test a network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_zoSojLuENq"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "iterator = tqdm(validation_dataloader, disable=True)\n",
    "torch.set_printoptions(threshold=327680)\n",
    "for datas, label_class, label_pose in iterator:\n",
    "        datas = datas.squeeze().to(device)\n",
    "        label_class = label_class.squeeze().to(device)\n",
    "        label_pose = label_pose.squeeze().to(device)\n",
    "        #print(\"Batch Data Shape:\", datas)\n",
    "        #print(\"Batch Class Labels Shape:\", label_class.shape)\n",
    "        #print(\"Batch Pose Labels Shape:\", label_pose.shape)\n",
    "        #print(label_class)\n",
    "        print(label_pose)\n",
    "\n",
    "#print(\"Length of Train Dataset:\", len(train_dataset))\n",
    "#print(\"Length of Validation Dataset:\", len(validation_dataset))\n",
    "#print(\"Length of Test Dataset:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Q1hcPs_tCOt"
   },
   "source": [
    "# **ORION nets**\n",
    "**Definition of the ORION net with the same architecture provided into the paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kWt8qhNntSYm"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module, Sequential, Conv3d, BatchNorm3d, ReLU, Dropout3d, MaxPool3d, Linear, LeakyReLU, Dropout, ELU\n",
    "\n",
    "# Define a simple CNN architecture\n",
    "class ORION(Module):\n",
    "    def __init__(self, num_classes, num_poses):\n",
    "        super().__init__()\n",
    "        self.model = Sequential(\n",
    "          # Definition of Conv1\n",
    "          Conv3d(in_channels=1, out_channels=32, kernel_size=5, stride=2),\n",
    "          BatchNorm3d(num_features=32),\n",
    "          ReLU(),\n",
    "          Dropout3d(p=0.2),\n",
    "\n",
    "          # Definition of Conv2\n",
    "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=64),\n",
    "          ReLU(),\n",
    "          MaxPool3d(kernel_size=2, stride=2),\n",
    "          Dropout3d(p=0.3),\n",
    "          )\n",
    "        #fc layer\n",
    "        self.fc1 = Sequential(\n",
    "            Linear(64*6*6*6, out_features=128),\n",
    "            ReLU(),\n",
    "            Dropout(p=0.4)\n",
    "        )\n",
    "        #classification of class and pose\n",
    "        self.class_layer = Linear(128, num_classes)\n",
    "        self.pose_layer = Linear(128, num_poses)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x).reshape((x.shape[0], -1))\n",
    "        x = self.fc1(x)\n",
    "        class_output = self.class_layer(x)\n",
    "        pose_output = self.pose_layer(x)\n",
    "        return class_output, pose_output\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv3d):\n",
    "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.pose_layer.weight, std=0.01)\n",
    "\n",
    "        if self.class_layer.bias is not None:\n",
    "            self.class_layer.bias.data.zero_()\n",
    "        if self.pose_layer.bias is not None:\n",
    "            self.pose_layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwmh9bUQtmQq"
   },
   "source": [
    "**Modified versions of the ORION net**\n",
    "\n",
    "**Extended ORION (from Paper)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WG9hazq5tqqJ"
   },
   "outputs": [],
   "source": [
    "# Define a simple CNN architecture\n",
    "class EXTENDED_ORION(Module):\n",
    "    def __init__(self, num_classes, num_poses):\n",
    "        super().__init__()\n",
    "        self.model = Sequential(\n",
    "          # Definition of Conv1\n",
    "          Conv3d(in_channels=1, out_channels=32, kernel_size=3, stride=2),\n",
    "          BatchNorm3d(num_features=32),\n",
    "          ReLU(),\n",
    "          Dropout3d(p=0.2),\n",
    "\n",
    "          # Definition of Conv2\n",
    "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=64),\n",
    "          ReLU(),\n",
    "          Dropout3d(p=0.3),\n",
    "\n",
    "          # Definition of Conv3\n",
    "          Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=128),\n",
    "          ReLU(),\n",
    "          Dropout3d(p=0.4),\n",
    "\n",
    "          # Definition of Conv4\n",
    "          Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=256),\n",
    "          ReLU(),\n",
    "          MaxPool3d(kernel_size=2, stride=2),\n",
    "          Dropout3d(p=0.6),\n",
    "          )\n",
    "\n",
    "        self.fc1 = Sequential(\n",
    "            Linear(256*4*4*4, out_features=128),\n",
    "            ReLU(),\n",
    "            Dropout(p=0.4)\n",
    "        )\n",
    "        self.class_layer = Linear(128, num_classes)\n",
    "        self.pose_layer = Linear(128, num_poses)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x).reshape((x.shape[0], -1))\n",
    "        x = self.fc1(x)\n",
    "        class_output = self.class_layer(x)\n",
    "        pose_output = self.pose_layer(x)\n",
    "        return class_output, pose_output\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv3d):\n",
    "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.pose_layer.weight, std=0.01)\n",
    "\n",
    "        if self.class_layer.bias is not None:\n",
    "            self.class_layer.bias.data.zero_()\n",
    "\n",
    "        if self.pose_layer.bias is not None:\n",
    "            self.pose_layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnFHls8TJnRE"
   },
   "source": [
    "**Using Leaky Relu as activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mLbQzCVRIbK8"
   },
   "outputs": [],
   "source": [
    "# Define a simple CNN architecture\n",
    "class ORION_LRELU(Module):\n",
    "    def __init__(self, num_classes, num_poses):\n",
    "        super().__init__()\n",
    "        self.model = Sequential(\n",
    "          # Definition of Conv1\n",
    "          Conv3d(in_channels=1, out_channels=32, kernel_size=5, stride=2),\n",
    "          BatchNorm3d(num_features=32),\n",
    "          LeakyReLU(negative_slope=0.1),\n",
    "          Dropout3d(p=0.2),\n",
    "\n",
    "          # Definition of Conv2\n",
    "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=64),\n",
    "          LeakyReLU(negative_slope=0.1),\n",
    "          MaxPool3d(kernel_size=2, stride=2),\n",
    "          Dropout3d(p=0.3),\n",
    "          )\n",
    "\n",
    "        self.fc1 = Sequential(\n",
    "            Linear(64*6*6*6, out_features=128),\n",
    "            ReLU(),\n",
    "            Dropout(p=0.4)\n",
    "        )\n",
    "        self.class_layer = Linear(128, num_classes)\n",
    "        self.pose_layer = Linear(128, num_poses)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x).reshape((x.shape[0], -1))\n",
    "        x = self.fc1(x)\n",
    "        class_output = self.class_layer(x)\n",
    "        pose_output = self.pose_layer(x)\n",
    "        return class_output, pose_output\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv3d):\n",
    "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.pose_layer.weight, std=0.01)\n",
    "\n",
    "        if self.class_layer.bias is not None:\n",
    "            self.class_layer.bias.data.zero_()\n",
    "\n",
    "        if self.pose_layer.bias is not None:\n",
    "            self.pose_layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0_ByOmzbI3Yf"
   },
   "outputs": [],
   "source": [
    "# Define a simple CNN architecture\n",
    "class EXTENDED_ORION_LRELU(Module):\n",
    "    def __init__(self, num_classes, num_poses):\n",
    "        super().__init__()\n",
    "        self.model = Sequential(\n",
    "          # Definition of Conv1\n",
    "          Conv3d(in_channels=1, out_channels=32, kernel_size=3, stride=2),\n",
    "          BatchNorm3d(num_features=32),\n",
    "          LeakyReLU(negative_slope=0.1),\n",
    "          Dropout3d(p=0.2),\n",
    "\n",
    "          # Definition of Conv2\n",
    "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=64),\n",
    "          LeakyReLU(negative_slope=0.1),\n",
    "          Dropout3d(p=0.3),\n",
    "\n",
    "          # Definition of Conv3\n",
    "          Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=128),\n",
    "          LeakyReLU(negative_slope=0.1),\n",
    "          Dropout3d(p=0.4),\n",
    "\n",
    "          # Definition of Conv4\n",
    "          Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=256),\n",
    "          LeakyReLU(negative_slope=0.1),\n",
    "          MaxPool3d(kernel_size=2, stride=2),\n",
    "          Dropout3d(p=0.6),\n",
    "          )\n",
    "\n",
    "        self.fc1 = Sequential(\n",
    "            Linear(256*4*4*4, out_features=128),\n",
    "            ReLU(),\n",
    "            Dropout(p=0.4)\n",
    "        )\n",
    "        self.class_layer = Linear(128, num_classes)\n",
    "        self.pose_layer = Linear(128, num_poses)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x).reshape((x.shape[0], -1))\n",
    "        x = self.fc1(x)\n",
    "        class_output = self.class_layer(x)\n",
    "        pose_output = self.pose_layer(x)\n",
    "        return class_output, pose_output\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv3d):\n",
    "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.pose_layer.weight, std=0.01)\n",
    "\n",
    "        if self.class_layer.bias is not None:\n",
    "            self.class_layer.bias.data.zero_()\n",
    "\n",
    "        if self.pose_layer.bias is not None:\n",
    "            self.pose_layer.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJGJsPyOJv39"
   },
   "source": [
    "**Using Exponential- linear unit as activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kyO6Wu4eJStM"
   },
   "outputs": [],
   "source": [
    "# Define a simple CNN architecture\n",
    "class ORION_ELU(Module):\n",
    "    def __init__(self, num_classes, num_poses):\n",
    "        super().__init__()\n",
    "        self.model = Sequential(\n",
    "          # Definition of Conv1\n",
    "          Conv3d(in_channels=1, out_channels=32, kernel_size=5, stride=2),\n",
    "          BatchNorm3d(num_features=32),\n",
    "          ELU(),\n",
    "          Dropout3d(p=0.2),\n",
    "\n",
    "          # Definition of Conv2\n",
    "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=64),\n",
    "          ELU(),\n",
    "          MaxPool3d(kernel_size=2, stride=2),\n",
    "          Dropout3d(p=0.3),\n",
    "          )\n",
    "\n",
    "        self.fc1 = Sequential(\n",
    "            Linear(64*6*6*6, out_features=128),\n",
    "            ReLU(),\n",
    "            Dropout(p=0.4)\n",
    "        )\n",
    "        self.class_layer = Linear(128, num_classes)\n",
    "        self.pose_layer = Linear(128, num_poses)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x).reshape((x.shape[0], -1))\n",
    "        x = self.fc1(x)\n",
    "        class_output = self.class_layer(x)\n",
    "        pose_output = self.pose_layer(x)\n",
    "        return class_output, pose_output\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv3d):\n",
    "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.pose_layer.weight, std=0.01)\n",
    "\n",
    "        if self.class_layer.bias is not None:\n",
    "            self.class_layer.bias.data.zero_()\n",
    "\n",
    "        if self.pose_layer.bias is not None:\n",
    "            self.pose_layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "o9dPMjFyJcB_"
   },
   "outputs": [],
   "source": [
    "# Define a simple CNN architecture\n",
    "class EXTENDED_ORION_ELU(Module):\n",
    "    def __init__(self, num_classes, num_poses):\n",
    "        super().__init__()\n",
    "        self.model = Sequential(\n",
    "          # Definition of Conv1\n",
    "          Conv3d(in_channels=1, out_channels=32, kernel_size=3, stride=2),\n",
    "          BatchNorm3d(num_features=32),\n",
    "          ELU(),\n",
    "          Dropout3d(p=0.2),\n",
    "\n",
    "          # Definition of Conv2\n",
    "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=64),\n",
    "          ELU(),\n",
    "          Dropout3d(p=0.3),\n",
    "\n",
    "          # Definition of Conv3\n",
    "          Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=128),\n",
    "          ELU(),\n",
    "          Dropout3d(p=0.4),\n",
    "\n",
    "          # Definition of Conv4\n",
    "          Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=256),\n",
    "          ELU(),\n",
    "          MaxPool3d(kernel_size=2, stride=2),\n",
    "          Dropout3d(p=0.6),\n",
    "          )\n",
    "\n",
    "        self.fc1 = Sequential(\n",
    "            Linear(256*4*4*4, out_features=128),\n",
    "            ReLU(),\n",
    "            Dropout(p=0.4)\n",
    "        )\n",
    "        self.class_layer = Linear(128, num_classes)\n",
    "        self.pose_layer = Linear(128, num_poses)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x).reshape((x.shape[0], -1))\n",
    "        x = self.fc1(x)\n",
    "        class_output = self.class_layer(x)\n",
    "        pose_output = self.pose_layer(x)\n",
    "        return class_output, pose_output\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv3d):\n",
    "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.pose_layer.weight, std=0.01)\n",
    "\n",
    "        if self.class_layer.bias is not None:\n",
    "            self.class_layer.bias.data.zero_()\n",
    "\n",
    "        if self.pose_layer.bias is not None:\n",
    "            self.pose_layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwOOH6QHjHDa"
   },
   "source": [
    "**Only classes evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0C0DnZ2ZjNOW"
   },
   "outputs": [],
   "source": [
    "# Define a simple CNN architecture\n",
    "class ORION_OC(Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = Sequential(\n",
    "          # Definition of Conv1\n",
    "          Conv3d(in_channels=1, out_channels=32, kernel_size=5, stride=2),\n",
    "          BatchNorm3d(num_features=32),\n",
    "          ReLU(),\n",
    "          Dropout3d(p=0.2),\n",
    "\n",
    "          # Definition of Conv2\n",
    "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=64),\n",
    "          ReLU(),\n",
    "          MaxPool3d(kernel_size=2, stride=2),\n",
    "          Dropout3d(p=0.3),\n",
    "          )\n",
    "\n",
    "        self.fc1 = Sequential(\n",
    "            Linear(64*6*6*6, out_features=128),\n",
    "            ReLU(),\n",
    "            Dropout(p=0.4)\n",
    "        )\n",
    "        self.class_layer = Linear(128, num_classes)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x).reshape((x.shape[0], -1))\n",
    "        x = self.fc1(x)\n",
    "        class_output = self.class_layer(x)\n",
    "        return class_output\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv3d):\n",
    "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
    "\n",
    "        if self.class_layer.bias is not None:\n",
    "            self.class_layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZwRT9aKClAs_"
   },
   "outputs": [],
   "source": [
    "# Define a simple CNN architecture\n",
    "class EXTENDED_ORION_OC(Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = Sequential(\n",
    "          # Definition of Conv1\n",
    "          Conv3d(in_channels=1, out_channels=32, kernel_size=3, stride=2),\n",
    "          BatchNorm3d(num_features=32),\n",
    "          ReLU(),\n",
    "          Dropout3d(p=0.2),\n",
    "\n",
    "          # Definition of Conv2\n",
    "          Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=64),\n",
    "          ReLU(),\n",
    "          Dropout3d(p=0.3),\n",
    "\n",
    "          # Definition of Conv3\n",
    "          Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=128),\n",
    "          ReLU(),\n",
    "          Dropout3d(p=0.4),\n",
    "\n",
    "          # Definition of Conv4\n",
    "          Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
    "          BatchNorm3d(num_features=256),\n",
    "          ReLU(),\n",
    "          MaxPool3d(kernel_size=2, stride=2),\n",
    "          Dropout3d(p=0.6),\n",
    "          )\n",
    "\n",
    "        self.fc1 = Sequential(\n",
    "            Linear(256*4*4*4, out_features=128),\n",
    "            ReLU(),\n",
    "            Dropout(p=0.4)\n",
    "        )\n",
    "        self.class_layer = Linear(128, num_classes)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x).reshape((x.shape[0], -1))\n",
    "        x = self.fc1(x)\n",
    "        class_output = self.class_layer(x)\n",
    "        return class_output\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, torch.nn.Conv3d):\n",
    "            torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        torch.nn.init.normal_(self.class_layer.weight, std=0.01)\n",
    "\n",
    "        if self.class_layer.bias is not None:\n",
    "            self.class_layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54IMyhKdtrNa"
   },
   "source": [
    "#**Performances evaluation functions**\n",
    "**Accuracies function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-XS1JjsIt26X"
   },
   "outputs": [],
   "source": [
    "def accuracy (network, dataloader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    network.to(device)\n",
    "    softmax= torch.nn.Softmax(dim=1)\n",
    "\n",
    "    iterator = tqdm(dataloader, disable=True)\n",
    "    with torch.no_grad():\n",
    "      #initialize empty arrays\n",
    "      predicted_class_array = []\n",
    "      true_class_array = []\n",
    "      predicted_pose_array = []\n",
    "      true_pose_array = []\n",
    "\n",
    "      # iterating through the batches\n",
    "      for datas, label_class, label_pose in iterator:\n",
    "        datas = datas.to(device)\n",
    "        label_class = label_class.squeeze().to(device) #remove the dimesions of size 1\n",
    "        label_pose = label_pose.squeeze().to(device)\n",
    "        network.eval()\n",
    "        # forward pass\n",
    "        predicted_class, predicted_pose = network(datas)\n",
    "\n",
    "        # obtain the class from the output\n",
    "        sum_class_pred = torch.sum(predicted_class, dim=0)\n",
    "        predicted_class = torch.argmax(sum_class_pred)\n",
    "        # obtain the pose label from the output\n",
    "        predicted_pose = torch.argmax(predicted_pose, dim=1)\n",
    "        #update the arrays concatenating the results and the true labels\n",
    "        predicted_class_array.append(predicted_class)\n",
    "        true_class_array.append(label_class[0]) #only the first element is important, the others are just repetitions due to the fact that each batch is composed by rotations of the same object\n",
    "        predicted_pose_array.append(predicted_pose)\n",
    "        true_pose_array.append(label_pose) #in this case all the labels are important\n",
    "\n",
    "      #build a single vector\n",
    "      true_class_array= torch.stack(true_class_array)\n",
    "      predicted_class_array= torch.stack(predicted_class_array)\n",
    "      true_pose_array = torch.cat(true_pose_array, axis=0)\n",
    "      predicted_pose_array= torch.cat(predicted_pose_array, dim=0)\n",
    "\n",
    "      # calculate accuracy\n",
    "      class_accuracy = torch.sum(predicted_class_array == true_class_array)/len(true_class_array)\n",
    "      pose_accuracy = torch.sum(predicted_pose_array == true_pose_array)/len(true_pose_array)\n",
    "\n",
    "      #return all the obtained elements\n",
    "      return class_accuracy, pose_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TENX4NNYuhyU"
   },
   "source": [
    "**Losses function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9wQRs_1Gt3A-"
   },
   "outputs": [],
   "source": [
    "def loss (network, dataloader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    network.to(device)\n",
    "    # defining the loss functions, creoss-entropy losses are chosen like in the paper\n",
    "    loss_fn_class = CrossEntropyLoss()\n",
    "    loss_fn_pose = CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    iterator = tqdm(dataloader, disable=True)\n",
    "    with torch.no_grad():\n",
    "      predicted_class_array = []\n",
    "      true_class_array = []\n",
    "      predicted_pose_array = []\n",
    "      true_pose_array = []\n",
    "\n",
    "      # iterating through the batches\n",
    "      for datas, label_class, label_pose in iterator:\n",
    "        datas = datas.to(device)\n",
    "        label_class = label_class.squeeze().to(device) #remove the dimesions of size 1\n",
    "        label_pose = label_pose.squeeze().to(device)\n",
    "        network.eval()\n",
    "        # forward pass\n",
    "        predicted_class, predicted_pose = network(datas)\n",
    "\n",
    "        predicted_class_array.append(predicted_class)\n",
    "        true_class_array.append(label_class) #only the first element is import, the others are just repetions due to the fact that each batch is composed by rotations of the same object\n",
    "        predicted_pose_array.append(predicted_pose)\n",
    "        true_pose_array.append(label_pose) #in this case all the labels are important\n",
    "\n",
    "      #build a single vector\n",
    "      true_class_array= torch.cat(true_class_array, axis=0)\n",
    "      predicted_class_array= torch.cat(predicted_class_array, axis=0)\n",
    "      true_pose_array = torch.cat(true_pose_array, axis=0)\n",
    "      predicted_pose_array = torch.cat(predicted_pose_array, axis=0)\n",
    "\n",
    "      #calculate losses\n",
    "      pose_loss = loss_fn_pose(predicted_pose_array, true_pose_array)\n",
    "      class_loss = loss_fn_class(predicted_class_array, true_class_array)\n",
    "      total_loss = (class_loss + pose_loss)/2 #weighted sum as defined in the paper\n",
    "\n",
    "      #return all the obtained elements\n",
    "      return class_loss, pose_loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzjYU44ynbUa"
   },
   "source": [
    "**Accuracies function for class classificatio only**\n",
    "Here we don't use the pose information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4RBF1qvXng7R"
   },
   "outputs": [],
   "source": [
    "def accuracy_OC (network, dataloader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    network.to(device)\n",
    "    softmax= torch.nn.Softmax(dim=1)\n",
    "\n",
    "    iterator = tqdm(dataloader, disable=True)\n",
    "    with torch.no_grad():\n",
    "      predicted_class_array = []\n",
    "      true_class_array = []\n",
    "      # iterating through the batches\n",
    "      for datas, label_class, label_pose in iterator:\n",
    "        datas = datas.to(device)\n",
    "        label_class = label_class.squeeze().to(device) #remove the dimesions of size 1\n",
    "        label_pose = label_pose.squeeze().to(device)\n",
    "        network.eval()\n",
    "        # forward pass\n",
    "        predicted_class = network(datas)\n",
    "\n",
    "        # obtain the class from the output\n",
    "        sum_class_pred = torch.sum(predicted_class, dim=0)\n",
    "        predicted_class = torch.argmax(sum_class_pred)\n",
    "        #update the arrays concatenating the results and the true labels\n",
    "        predicted_class_array.append(predicted_class)\n",
    "        true_class_array.append(label_class[0]) #only the first element is important, the others are just repetitions due to the fact that each batch is composed by rotations of the same object\n",
    "\n",
    "      #build a single vector\n",
    "      true_class_array= torch.stack(true_class_array)\n",
    "      predicted_class_array= torch.stack(predicted_class_array)\n",
    "\n",
    "      # calculate accuracy\n",
    "      class_accuracy = torch.sum(predicted_class_array == true_class_array)/len(true_class_array)\n",
    "\n",
    "      #return all the obtained elements\n",
    "      return class_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31wgdtUfnvbe"
   },
   "source": [
    "**Losses function for class classification only**\n",
    "Here we don't use the pose information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-A3dZbovn2qN"
   },
   "outputs": [],
   "source": [
    "def loss_OC (network, dataloader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    network.to(device)\n",
    "    # defining the loss functions, creoss-entropy losses are chosen like in the paper\n",
    "    loss_fn_class = CrossEntropyLoss()\n",
    "\n",
    "    iterator = tqdm(dataloader, disable=True)\n",
    "    with torch.no_grad():\n",
    "      predicted_class_array = []\n",
    "      true_class_array = []\n",
    "\n",
    "      # iterating through the batches\n",
    "      for datas, label_class, label_pose in iterator:\n",
    "        datas = datas.to(device)\n",
    "        label_class = label_class.squeeze().to(device) #remove the dimesions of size 1\n",
    "        label_pose = label_pose.squeeze().to(device)\n",
    "        network.eval()\n",
    "        # forward pass\n",
    "        predicted_class = network(datas)\n",
    "\n",
    "        predicted_class_array.append(predicted_class)\n",
    "        true_class_array.append(label_class) #only the first element is import, the others are just repetions due to the fact that each batch is composed by rotations of the same object\n",
    "\n",
    "      #build a single vector\n",
    "      true_class_array= torch.cat(true_class_array, axis=0)\n",
    "      predicted_class_array= torch.cat(predicted_class_array, axis=0)\n",
    "\n",
    "      #calculate losses\n",
    "      class_loss = loss_fn_class(predicted_class_array, true_class_array)\n",
    "\n",
    "      #return all the obtained elements\n",
    "      return class_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMz1NguFNtGm"
   },
   "source": [
    "#**Data logging**\n",
    "This section is needed in order to acquire graphical datas. It's not needed to train and test a network. However the run is mandatory in our case since wandb is also implemented into the training process. If it's not made the training process give an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "FsmpLwSfNwLN",
    "outputId": "bcadc33a-3be3-451e-c666-cb339ffe68d2"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20240116_191918-k1ituxwv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gigia/Wandb_accuracy_SDGvsAdam/runs/k1ituxwv' target=\"_blank\">decent-totem-1</a></strong> to <a href='https://wandb.ai/gigia/Wandb_accuracy_SDGvsAdam' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gigia/Wandb_accuracy_SDGvsAdam' target=\"_blank\">https://wandb.ai/gigia/Wandb_accuracy_SDGvsAdam</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gigia/Wandb_accuracy_SDGvsAdam/runs/k1ituxwv' target=\"_blank\">https://wandb.ai/gigia/Wandb_accuracy_SDGvsAdam/runs/k1ituxwv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_metric.Metric at 0x7febafe14790>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Wandb_accuracy_SDGvsAdam\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"dataset\": \"ModelNet10\",\n",
    "    \"epochs\": 200,\n",
    "    }\n",
    ")\n",
    "wandb.define_metric(\"epoch\")\n",
    "\n",
    "wandb.define_metric(\"validation_loss_SDG\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"class_validation_loss_SDG\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"pose_validation_loss_SDG\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"class_validation_accuracy_SDG\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"pose_validation_accuracy_SDG\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"class_validation_error_SDG\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"pose_validation_error_SDG\", step_metric=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Glm0Id1Ixfel"
   },
   "source": [
    "**Run this to deal only with classes and not pose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "d37fff90090d46dfaafe446b5a184a28",
      "7bf85fef08e7415d8c3d81bbb1fb82a1",
      "21813713d6a94bc09eaadbe6cf385a97",
      "5d180d4bbfb844f39893074fc64f9ab5",
      "fcf89d1499fa4332a6850196d06d936f",
      "bdcc9b73bfc74d66b530ef9d36d471d0",
      "f5a62c36974a404cbc3ce16065d9a525",
      "006a2286517a4ac2b2ebec195c29ac27"
     ]
    },
    "id": "u81-B3fCxjyL",
    "outputId": "94c9b41d-e7d2-4088-a29b-766bf586436d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:sy89fnlb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37fff90090d46dfaafe446b5a184a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-smoke-35</strong> at: <a href='https://wandb.ai/gigia/Modelnet10_multiple/runs/sy89fnlb' target=\"_blank\">https://wandb.ai/gigia/Modelnet10_multiple/runs/sy89fnlb</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240108_202505-sy89fnlb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:sy89fnlb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20240108_202507-q29w93id</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gigia/Modelnet10_onlyclassesSDG/runs/q29w93id' target=\"_blank\">desert-cloud-23</a></strong> to <a href='https://wandb.ai/gigia/Modelnet10_onlyclassesSDG' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gigia/Modelnet10_onlyclassesSDG' target=\"_blank\">https://wandb.ai/gigia/Modelnet10_onlyclassesSDG</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gigia/Modelnet10_onlyclassesSDG/runs/q29w93id' target=\"_blank\">https://wandb.ai/gigia/Modelnet10_onlyclassesSDG/runs/q29w93id</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_metric.Metric at 0x7db3f520d4b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new wandb\n",
    "wandb.init(\n",
    "\n",
    "    project=\"Modelnet10_onlyclassesSDG\",\n",
    "\n",
    "    config={\n",
    "    \"dataset\": \"ModelNet10\",\n",
    "    \"epochs\": 200,\n",
    "    }\n",
    ")\n",
    "wandb.define_metric(\"epoch\")\n",
    "wandb.define_metric(\"class_validation_loss\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"class_validation_accuracy\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"class_validation_error\", step_metric=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy5R44UNuwKo"
   },
   "source": [
    "# **Training process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "k8BG3qy9u1x9"
   },
   "outputs": [],
   "source": [
    "def training(model,train_data_loading, validation_data_loading, epochs, opt, early_stopping):\n",
    "\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  model.to(device)\n",
    "  best_loss=np.inf #set the best value of the loss to infinite\n",
    "  early_stopping_val=0\n",
    "  class_loss_function = CrossEntropyLoss()\n",
    "  pose_loss_function = CrossEntropyLoss()\n",
    "\n",
    "  #start loop for each epoch, defined in input\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    model.train()\n",
    "    elements = tqdm(train_data_loading)\n",
    "    for datas, label_class, label_pose in elements:\n",
    "      #input real labels\n",
    "      datas=datas.to(device)\n",
    "      label_class=label_class.squeeze().to(device)\n",
    "      label_pose=label_pose.squeeze().to(device)\n",
    "      #do the forward pass\n",
    "      label_class_prediction, label_pose_prediction=model(datas)\n",
    "\n",
    "      #calculate losses\n",
    "      class_loss = class_loss_function(label_class_prediction, label_class)\n",
    "      pose_loss = pose_loss_function(label_pose_prediction, label_pose)\n",
    "      train_loss = (class_loss + pose_loss)/2\n",
    "      # logging training losses\n",
    "      wandb.log({\"train_loss\": train_loss, \"pose_train_loss\": pose_loss, \"class_train_loss\": class_loss })\n",
    "\n",
    "      #backword pass\n",
    "      opt.zero_grad()\n",
    "      train_loss.backward()\n",
    "      opt.step()\n",
    "      #view batch results\n",
    "      elements.set_description(f\"Train loss: {train_loss.detach().cpu().numpy()} class loss: {class_loss.detach().cpu().numpy()}  pose loss: {pose_loss.detach().cpu().numpy()}\")\n",
    "    #compute accuracies and losses on validation set\n",
    "    class_accuracy_val, pose_accuracy_val = accuracy(model,validation_data_loading)\n",
    "    class_loss_val, pose_loss_val, total_loss_val=loss(model, validation_data_loading)\n",
    "\n",
    "    # logging validation results\n",
    "    wandb.log({\"epoch\": epoch+1,\n",
    "                \"validation_loss\": total_loss_val,\n",
    "                \"pose_validation_loss\": pose_loss_val,\n",
    "                \"class_validation_loss\": class_loss_val,\n",
    "                \"class_validation_accuracy\": class_accuracy_val,\n",
    "                \"pose_validation_accuracy\": pose_accuracy_val,\n",
    "                \"class_validation_error\": 1-class_accuracy_val,\n",
    "                \"pose_validation_error\": 1-pose_accuracy_val})\n",
    "\n",
    "    #display elements\n",
    "    print(f\"class accuracy: {class_accuracy_val}, \\n pose accuracy: {pose_accuracy_val}\")\n",
    "    print(f\"class loss: {class_loss_val}, \\n pose loss: {pose_loss} , \\n total loss: {total_loss_val}\")\n",
    "    #updating model looking at the loss\n",
    "    if total_loss_val < best_loss:\n",
    "        print(\"Saved Update Model\")\n",
    "        #print(model)\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        best_loss = total_loss_val\n",
    "        early_stopping_val=0 #re-initialize the count\n",
    "    else:\n",
    "        early_stopping_val+=1\n",
    "        #check early stopping and exit training loop\n",
    "    if early_stopping_val >= early_stopping :\n",
    "      print(\"EARLY STOPPING\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-MjlcZOmnfp"
   },
   "source": [
    "**Define a training process for the usage of classes only, without considering the orientation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vJAddDoUml2q"
   },
   "outputs": [],
   "source": [
    "def training_OC(model,train_data_loading, validation_data_loading, epochs, opt, early_stopping):\n",
    "\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  model.to(device)\n",
    "  best_loss=np.inf #set the best value of the loss to infinite\n",
    "  early_stopping_val=0\n",
    "  class_loss_function = CrossEntropyLoss()\n",
    "\n",
    "  #start loop for each epoch, defined in input\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    model.train()\n",
    "    elements = tqdm(train_data_loading)\n",
    "    for datas, label_class, label_pose in elements:\n",
    "      #input real labels\n",
    "      datas=datas.to(device)\n",
    "      label_class=label_class.squeeze().to(device)\n",
    "      label_pose=label_pose.squeeze().to(device)\n",
    "      #do the forward pass\n",
    "      label_class_prediction=model(datas)\n",
    "\n",
    "      #calculate losses\n",
    "      class_loss = class_loss_function(label_class_prediction, label_class)\n",
    "\n",
    "\n",
    "      #backword pass\n",
    "      opt.zero_grad()\n",
    "      class_loss.backward()\n",
    "      opt.step()\n",
    "      #view batch results\n",
    "      elements.set_description(f\"class loss: {class_loss.detach().cpu().numpy()}\")\n",
    "    #compute accuracies and losses on validation set\n",
    "    class_accuracy_val = accuracy_OC(model,validation_data_loading)\n",
    "    class_loss_val = loss_OC(model, validation_data_loading)\n",
    "    # logging validation results\n",
    "    wandb.log({\"epoch\": epoch+1,\n",
    "                \"class_validation_loss\": class_loss_val,\n",
    "                \"class_validation_accuracy\": class_accuracy_val,\n",
    "                \"class_validation_error\": 1-class_accuracy_val})\n",
    "\n",
    "    #display elements\n",
    "    print(f\"class accuracy: {class_accuracy_val}\")\n",
    "    print(f\"class loss: {class_loss_val}\")\n",
    "    #updating model looking at the loss\n",
    "    if class_loss_val < best_loss:\n",
    "        print(\"Saved Model\")\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        best_loss = class_loss_val\n",
    "        early_stopping_val=0 #re-initialize the count\n",
    "    else:\n",
    "        early_stopping_val+=1\n",
    "        #check early stopping and exit training loop\n",
    "    if early_stopping_val >= early_stopping :\n",
    "      print(\"EARLY STOPPING\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0IDf9iOvykr"
   },
   "source": [
    "# **Initialization and running**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2f7387ce9bd74bdfa1245dfa4f1d13d5",
      "9bfb20522984450f9c5287a72d950a00",
      "bce2328357954d129f90cf1b49e23616",
      "f033a25642d04bbf9f61ec24a8a68290",
      "bad0a52044bf4c819a4f3a740544d8fb",
      "cbee279762574f7db3cec3897bbda88a",
      "c00a4cd33c9b4e6187e007e9b39600a7",
      "b1777fe8826e49dfb6183545bfa9f977"
     ]
    },
    "id": "81j1m4N8vw14",
    "outputId": "88f3f0dd-56d1-400a-98b8-2330408acf78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7249283194541931 class loss: 0.49175459146499634  pose loss: 0.9581020474433899: 100%|██████████| 1371/1371 [00:36<00:00, 37.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.8592493534088135, \n",
      " pose accuracy: 0.6902368068695068\n",
      "class loss: 0.5240792632102966, \n",
      " pose loss: 0.9581020474433899 , \n",
      " total loss: 0.8158986568450928\n",
      "Saved Update Model\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.37988850474357605 class loss: 0.237222358584404  pose loss: 0.5225546360015869: 100%|██████████| 1371/1371 [00:34<00:00, 40.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.8686327338218689, \n",
      " pose accuracy: 0.741510272026062\n",
      "class loss: 0.4354113042354584, \n",
      " pose loss: 0.5225546360015869 , \n",
      " total loss: 0.640637218952179\n",
      "Saved Update Model\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.42424383759498596 class loss: 0.21776558458805084  pose loss: 0.6307221055030823: 100%|██████████| 1371/1371 [00:35<00:00, 38.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.8873994946479797, \n",
      " pose accuracy: 0.7716711163520813\n",
      "class loss: 0.3795611560344696, \n",
      " pose loss: 0.6307221055030823 , \n",
      " total loss: 0.5590656995773315\n",
      "Saved Update Model\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5557166337966919 class loss: 0.4018164873123169  pose loss: 0.7096168398857117: 100%|██████████| 1371/1371 [00:36<00:00, 37.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9088472127914429, \n",
      " pose accuracy: 0.7870866656303406\n",
      "class loss: 0.35953205823898315, \n",
      " pose loss: 0.7096168398857117 , \n",
      " total loss: 0.5223706364631653\n",
      "Saved Update Model\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.34289103746414185 class loss: 0.19036200642585754  pose loss: 0.49542006850242615: 100%|██████████| 1371/1371 [00:36<00:00, 37.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.8981233239173889, \n",
      " pose accuracy: 0.7921134829521179\n",
      "class loss: 0.3655981719493866, \n",
      " pose loss: 0.49542006850242615 , \n",
      " total loss: 0.5125939846038818\n",
      "Saved Update Model\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.18908385932445526 class loss: 0.0554681196808815  pose loss: 0.3226996064186096: 100%|██████████| 1371/1371 [00:35<00:00, 39.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.904825747013092, \n",
      " pose accuracy: 0.7981456518173218\n",
      "class loss: 0.3605037033557892, \n",
      " pose loss: 0.3226996064186096 , \n",
      " total loss: 0.49895572662353516\n",
      "Saved Update Model\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.25659888982772827 class loss: 0.14115343987941742  pose loss: 0.37204432487487793: 100%|██████████| 1371/1371 [00:35<00:00, 38.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9128686785697937, \n",
      " pose accuracy: 0.8069705367088318\n",
      "class loss: 0.34341999888420105, \n",
      " pose loss: 0.37204432487487793 , \n",
      " total loss: 0.4783164858818054\n",
      "Saved Update Model\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3681327998638153 class loss: 0.16954787075519562  pose loss: 0.5667177438735962: 100%|██████████| 1371/1371 [00:35<00:00, 38.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9061662554740906, \n",
      " pose accuracy: 0.8111037015914917\n",
      "class loss: 0.3525443375110626, \n",
      " pose loss: 0.5667177438735962 , \n",
      " total loss: 0.4776291847229004\n",
      "Saved Update Model\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.402275025844574 class loss: 0.21057012677192688  pose loss: 0.5939798951148987: 100%|██████████| 1371/1371 [00:35<00:00, 39.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9115281701087952, \n",
      " pose accuracy: 0.817694365978241\n",
      "class loss: 0.33056992292404175, \n",
      " pose loss: 0.5939798951148987 , \n",
      " total loss: 0.45842406153678894\n",
      "Saved Update Model\n",
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6238417029380798 class loss: 0.39937666058540344  pose loss: 0.8483067154884338: 100%|██████████| 1371/1371 [00:36<00:00, 37.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.8994638323783875, \n",
      " pose accuracy: 0.8173592686653137\n",
      "class loss: 0.36768004298210144, \n",
      " pose loss: 0.8483067154884338 , \n",
      " total loss: 0.4813114404678345\n",
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.35124170780181885 class loss: 0.11799028515815735  pose loss: 0.5844931602478027: 100%|██████████| 1371/1371 [00:34<00:00, 39.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.904825747013092, \n",
      " pose accuracy: 0.8144548535346985\n",
      "class loss: 0.3662302494049072, \n",
      " pose loss: 0.5844931602478027 , \n",
      " total loss: 0.4795379936695099\n",
      "Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2293931394815445 class loss: 0.09862494468688965  pose loss: 0.36016133427619934: 100%|██████████| 1371/1371 [00:34<00:00, 39.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9021447896957397, \n",
      " pose accuracy: 0.8160187602043152\n",
      "class loss: 0.36320263147354126, \n",
      " pose loss: 0.36016133427619934 , \n",
      " total loss: 0.47312623262405396\n",
      "Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2463119924068451 class loss: 0.043416887521743774  pose loss: 0.4492070972919464: 100%|██████████| 1371/1371 [00:35<00:00, 38.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.900804340839386, \n",
      " pose accuracy: 0.8193699717521667\n",
      "class loss: 0.4023611843585968, \n",
      " pose loss: 0.4492070972919464 , \n",
      " total loss: 0.5012340545654297\n",
      "Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.16872631013393402 class loss: 0.09229663759469986  pose loss: 0.24515597522258759: 100%|██████████| 1371/1371 [00:34<00:00, 39.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9142091274261475, \n",
      " pose accuracy: 0.8185880184173584\n",
      "class loss: 0.36164525151252747, \n",
      " pose loss: 0.24515597522258759 , \n",
      " total loss: 0.47289812564849854\n",
      "Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2750675082206726 class loss: 0.11327078938484192  pose loss: 0.4368642568588257: 100%|██████████| 1371/1371 [00:36<00:00, 37.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.890080451965332, \n",
      " pose accuracy: 0.8201519250869751\n",
      "class loss: 0.3956986665725708, \n",
      " pose loss: 0.4368642568588257 , \n",
      " total loss: 0.4900384843349457\n",
      "Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3844509720802307 class loss: 0.12727421522140503  pose loss: 0.6416277289390564: 100%|██████████| 1371/1371 [00:35<00:00, 38.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.904825747013092, \n",
      " pose accuracy: 0.8240616917610168\n",
      "class loss: 0.39867666363716125, \n",
      " pose loss: 0.6416277289390564 , \n",
      " total loss: 0.4974048137664795\n",
      "Epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.22578541934490204 class loss: 0.17575430870056152  pose loss: 0.27581652998924255: 100%|██████████| 1371/1371 [00:35<00:00, 38.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.900804340839386, \n",
      " pose accuracy: 0.8212689757347107\n",
      "class loss: 0.38640058040618896, \n",
      " pose loss: 0.27581652998924255 , \n",
      " total loss: 0.4844296872615814\n",
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.12257373332977295 class loss: 0.027139050886034966  pose loss: 0.21800841391086578: 100%|██████████| 1371/1371 [00:35<00:00, 38.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9101877212524414, \n",
      " pose accuracy: 0.8297587037086487\n",
      "class loss: 0.3927100896835327, \n",
      " pose loss: 0.21800841391086578 , \n",
      " total loss: 0.4857643246650696\n",
      "Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08877583593130112 class loss: 0.018777865916490555  pose loss: 0.15877380967140198: 100%|██████████| 1371/1371 [00:35<00:00, 39.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.904825747013092, \n",
      " pose accuracy: 0.8287533521652222\n",
      "class loss: 0.40017691254615784, \n",
      " pose loss: 0.15877380967140198 , \n",
      " total loss: 0.4944225549697876\n",
      "Epoch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.21410876512527466 class loss: 0.10255323350429535  pose loss: 0.32566431164741516: 100%|██████████| 1371/1371 [00:34<00:00, 39.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.915549635887146, \n",
      " pose accuracy: 0.8223860859870911\n",
      "class loss: 0.3946186602115631, \n",
      " pose loss: 0.32566431164741516 , \n",
      " total loss: 0.49246007204055786\n",
      "Epoch: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.12757594883441925 class loss: 0.05464838817715645  pose loss: 0.20050349831581116: 100%|██████████| 1371/1371 [00:36<00:00, 38.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9115281701087952, \n",
      " pose accuracy: 0.8321045637130737\n",
      "class loss: 0.4023740589618683, \n",
      " pose loss: 0.20050349831581116 , \n",
      " total loss: 0.4931457042694092\n",
      "Epoch: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.25615453720092773 class loss: 0.2321365773677826  pose loss: 0.28017252683639526: 100%|██████████| 1371/1371 [00:35<00:00, 38.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9142091274261475, \n",
      " pose accuracy: 0.8325514197349548\n",
      "class loss: 0.42394474148750305, \n",
      " pose loss: 0.28017252683639526 , \n",
      " total loss: 0.5085355639457703\n",
      "Epoch: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.28410759568214417 class loss: 0.21865685284137726  pose loss: 0.34955835342407227: 100%|██████████| 1371/1371 [00:34<00:00, 39.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9101877212524414, \n",
      " pose accuracy: 0.8303172588348389\n",
      "class loss: 0.4239785373210907, \n",
      " pose loss: 0.34955835342407227 , \n",
      " total loss: 0.5087123513221741\n",
      "Epoch: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.35448282957077026 class loss: 0.22900603711605072  pose loss: 0.479959636926651: 100%|██████████| 1371/1371 [00:36<00:00, 37.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9168900847434998, \n",
      " pose accuracy: 0.8350089192390442\n",
      "class loss: 0.3901263177394867, \n",
      " pose loss: 0.479959636926651 , \n",
      " total loss: 0.4796660542488098\n",
      "Epoch: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.24191783368587494 class loss: 0.17560124397277832  pose loss: 0.30823442339897156: 100%|██████████| 1371/1371 [00:35<00:00, 38.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.8981233239173889, \n",
      " pose accuracy: 0.8317694664001465\n",
      "class loss: 0.4355712831020355, \n",
      " pose loss: 0.30823442339897156 , \n",
      " total loss: 0.5149421691894531\n",
      "Epoch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.26219120621681213 class loss: 0.14653916656970978  pose loss: 0.3778432607650757: 100%|██████████| 1371/1371 [00:36<00:00, 37.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9088472127914429, \n",
      " pose accuracy: 0.8325514197349548\n",
      "class loss: 0.4178560674190521, \n",
      " pose loss: 0.3778432607650757 , \n",
      " total loss: 0.503530740737915\n",
      "Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09720532596111298 class loss: 0.0039091468788683414  pose loss: 0.19050151109695435: 100%|██████████| 1371/1371 [00:34<00:00, 39.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.904825747013092, \n",
      " pose accuracy: 0.8306523561477661\n",
      "class loss: 0.39506444334983826, \n",
      " pose loss: 0.19050151109695435 , \n",
      " total loss: 0.4831441044807434\n",
      "Epoch: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.19371318817138672 class loss: 0.14366932213306427  pose loss: 0.24375706911087036: 100%|██████████| 1371/1371 [00:34<00:00, 39.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9142091274261475, \n",
      " pose accuracy: 0.8395889401435852\n",
      "class loss: 0.43033191561698914, \n",
      " pose loss: 0.24375706911087036 , \n",
      " total loss: 0.5078661441802979\n",
      "Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.35960590839385986 class loss: 0.25200510025024414  pose loss: 0.4672067165374756: 100%|██████████| 1371/1371 [00:36<00:00, 37.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class accuracy: 0.9021447896957397, \n",
      " pose accuracy: 0.8316577672958374\n",
      "class loss: 0.42805370688438416, \n",
      " pose loss: 0.4672067165374756 , \n",
      " total loss: 0.5041864514350891\n",
      "EARLY STOPPING\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7387ce9bd74bdfa1245dfa4f1d13d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>class_train_loss</td><td>▇▅█▅▃▄▅▂▃▃▃▂▃▃▂▂▂▃▁▃▂▂▁▄▁▁▂▂▂▁▁▂▁▂▂▃▁▁▁▁</td></tr><tr><td>class_validation_accuracy</td><td>▁▂▄▇▆▇█▇▇▆▇▆▆█▅▇▆▇▇█▇█▇█▆▇▇█▆</td></tr><tr><td>class_validation_error</td><td>█▇▅▂▃▂▁▂▂▃▂▃▃▁▄▂▃▂▂▁▂▁▂▁▃▂▂▁▃</td></tr><tr><td>class_validation_loss</td><td>█▅▃▂▂▂▁▂▁▂▂▂▄▂▃▃▃▃▄▃▄▄▄▃▅▄▃▅▅</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>pose_train_loss</td><td>█▅▅▅▃▄▄▂▂▂▃▂▃▂▃▃▁▂▂▂▂▂▁▃▁▂▁▂▂▁▂▂▁▃▂▂▂▁▁▂</td></tr><tr><td>pose_validation_accuracy</td><td>▁▃▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇█████████</td></tr><tr><td>pose_validation_error</td><td>█▆▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>pose_validation_loss</td><td>█▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▆▅▃▄▄▂▂▂▃▂▃▃▃▂▁▂▂▂▂▂▁▄▁▁▂▂▂▁▂▂▁▃▂▂▂▁▁▂</td></tr><tr><td>validation_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>class_train_loss</td><td>0.25201</td></tr><tr><td>class_validation_accuracy</td><td>0.90214</td></tr><tr><td>class_validation_error</td><td>0.09786</td></tr><tr><td>class_validation_loss</td><td>0.42805</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>pose_train_loss</td><td>0.46721</td></tr><tr><td>pose_validation_accuracy</td><td>0.83166</td></tr><tr><td>pose_validation_error</td><td>0.16834</td></tr><tr><td>pose_validation_loss</td><td>0.58032</td></tr><tr><td>train_loss</td><td>0.35961</td></tr><tr><td>validation_loss</td><td>0.50419</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-totem-1</strong> at: <a href='https://wandb.ai/gigia/Wandb_accuracy_SDGvsAdam/runs/k1ituxwv' target=\"_blank\">https://wandb.ai/gigia/Wandb_accuracy_SDGvsAdam/runs/k1ituxwv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240116_191918-k1ituxwv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "\n",
    "#manually define the architecture type\n",
    "chosen_model=1;\n",
    "#manually define the optimizer type\n",
    "chosen_opt=1;\n",
    "\n",
    "#define epochs and early stopping\n",
    "epochs=200\n",
    "es=20\n",
    "\n",
    "#ReLU\n",
    "if chosen_model==1 and chosen_opt==1:\n",
    "  model=ORION(num_classes, num_poses)\n",
    "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "elif chosen_model==1 and chosen_opt==2:\n",
    "  model=ORION(num_classes, num_poses)\n",
    "  opt = Adam(model.parameters(), lr=1e-4)\n",
    "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "elif chosen_model==2 and chosen_opt==1:\n",
    "  model=EXTENDED_ORION(num_classes, num_poses)\n",
    "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "elif chosen_model==2 and chosen_opt==2:\n",
    "  model=EXTENDED_ORION(num_classes, num_poses)\n",
    "  opt = Adam(model.parameters(), lr=1e-4)\n",
    "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "\n",
    "#Leaky Relu\n",
    "elif chosen_model==3 and chosen_opt==1:\n",
    "  model=ORION_LRELU(num_classes, num_poses)\n",
    "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "elif chosen_model==3 and chosen_opt==2:\n",
    "  model=ORION_LRELU(num_classes, num_poses)\n",
    "  opt = Adam(model.parameters(), lr=1e-4)\n",
    "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "elif chosen_model==4 and chosen_opt==1:\n",
    "  model=EXTENDED_ORION_LRELU(num_classes, num_poses)\n",
    "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "elif chosen_model==4 and chosen_opt==2:\n",
    "  model=EXTENDED_ORION_LRELU(num_classes, num_poses)\n",
    "  opt = Adam(model.parameters(), lr=1e-4)\n",
    "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "\n",
    "#ELU\n",
    "elif chosen_model==5 and chosen_opt==1:\n",
    "  model=ORION_ELU(num_classes, num_poses)\n",
    "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "elif chosen_model==5 and chosen_opt==2:\n",
    "  model=ORION_ELU(num_classes, num_poses)\n",
    "  opt = Adam(model.parameters(), lr=1e-4)\n",
    "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "elif chosen_model==6 and chosen_opt==1:\n",
    "  model=EXTENDED_ORION_ELU(num_classes, num_poses)\n",
    "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "elif chosen_model==6 and chosen_opt==2:\n",
    "  model=EXTENDED_ORION_ELU(num_classes, num_poses)\n",
    "  opt = Adam(model.parameters(), lr=1e-4)\n",
    "  training(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "\n",
    "#Only classes evalyuation\n",
    "elif chosen_model==7 and chosen_opt==1:\n",
    "  model=ORION_OC(num_classes)\n",
    "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True) #validation accuracy 94.67% test 90.75%\n",
    "  training_OC(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "elif chosen_model==7 and chosen_opt==2:\n",
    "  model=ORION_OC(num_classes)\n",
    "  opt = Adam(model.parameters(), lr=1e-4) #validation accuracy 93.33% test 90.86%\n",
    "  training_OC(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "elif chosen_model==8 and chosen_opt==1:\n",
    "  model=EXTENDED_ORION_OC(num_classes)\n",
    "  opt = SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "  training_OC(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "elif chosen_model==8 and chosen_opt==2:\n",
    "  model=EXTENDED_ORION_OC(num_classes)\n",
    "  opt = Adam(model.parameters(), lr=1e-4)\n",
    "  training_OC(model, train_dataloader, validation_dataloader, epochs, opt, es)\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "  print(\"Indeces are wrong\")\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUYP_76OyGz9"
   },
   "source": [
    "# **Testing Phase**\n",
    "This print the test and validation loss and accuracy. Notice that if the only class version is used, some little changes are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8jzoQBoyOS-",
    "outputId": "81534a50-152b-4cae-8a4a-8b52f5d3dbd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Class Accuracy : tensor(0.9115, device='cuda:0')\n",
      "Validation Pose Accuracy : tensor(0.8177, device='cuda:0')\n",
      "Test Class Accuracy : tensor(0.9395, device='cuda:0')\n",
      "Test Pose Accuracy : tensor(0.8340, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "\n",
    "val_class_accuracy, val_pose_accuracy = accuracy(model, validation_dataloader)\n",
    "#val_class_accuracy = accuracy_OC(model, validation_dataloader)\n",
    "\n",
    "print(\"Validation Class Accuracy : \"+str(val_class_accuracy))\n",
    "print(\"Validation Pose Accuracy : \"+str(val_pose_accuracy))\n",
    "\n",
    "test_class_accuracy, test_pose_accuracy = accuracy(model, test_dataloader)\n",
    "#test_class_accuracy = accuracy_OC(model, test_dataloader)\n",
    "\n",
    "\n",
    "print(\"Test Class Accuracy : \"+str(test_class_accuracy))\n",
    "print(\"Test Pose Accuracy : \"+str(test_pose_accuracy))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_Q1hcPs_tCOt",
    "54IMyhKdtrNa",
    "jy5R44UNuwKo"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "006a2286517a4ac2b2ebec195c29ac27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "21813713d6a94bc09eaadbe6cf385a97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5a62c36974a404cbc3ce16065d9a525",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_006a2286517a4ac2b2ebec195c29ac27",
      "value": 1
     }
    },
    "2f7387ce9bd74bdfa1245dfa4f1d13d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9bfb20522984450f9c5287a72d950a00",
       "IPY_MODEL_bce2328357954d129f90cf1b49e23616"
      ],
      "layout": "IPY_MODEL_f033a25642d04bbf9f61ec24a8a68290"
     }
    },
    "5d180d4bbfb844f39893074fc64f9ab5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bf85fef08e7415d8c3d81bbb1fb82a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fcf89d1499fa4332a6850196d06d936f",
      "placeholder": "​",
      "style": "IPY_MODEL_bdcc9b73bfc74d66b530ef9d36d471d0",
      "value": "0.011 MB of 0.011 MB uploaded\r"
     }
    },
    "9bfb20522984450f9c5287a72d950a00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bad0a52044bf4c819a4f3a740544d8fb",
      "placeholder": "​",
      "style": "IPY_MODEL_cbee279762574f7db3cec3897bbda88a",
      "value": "0.022 MB of 0.022 MB uploaded\r"
     }
    },
    "b1777fe8826e49dfb6183545bfa9f977": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bad0a52044bf4c819a4f3a740544d8fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bce2328357954d129f90cf1b49e23616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c00a4cd33c9b4e6187e007e9b39600a7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b1777fe8826e49dfb6183545bfa9f977",
      "value": 1
     }
    },
    "bdcc9b73bfc74d66b530ef9d36d471d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c00a4cd33c9b4e6187e007e9b39600a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbee279762574f7db3cec3897bbda88a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d37fff90090d46dfaafe446b5a184a28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7bf85fef08e7415d8c3d81bbb1fb82a1",
       "IPY_MODEL_21813713d6a94bc09eaadbe6cf385a97"
      ],
      "layout": "IPY_MODEL_5d180d4bbfb844f39893074fc64f9ab5"
     }
    },
    "f033a25642d04bbf9f61ec24a8a68290": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5a62c36974a404cbc3ce16065d9a525": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcf89d1499fa4332a6850196d06d936f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
